Developing effective virtual environment scenarios that involve human participants is a challenging task because of the difficulty associated with anticipating and responding to all possible reactions of the human participant as the scenario unfolds. Further complicating the task in immersive simulations is the need to accommodate multiple interaction modalities that go beyond direct keyboard and mouse input, examples of which include gestures, use of domain-specific props, and voice recognition. In this paper, we present an approach to modeling an immersive virtual environment aimed at training surgical procedures, the Virtual Operating Room (VOR). In the VOR, trainees interact with a surgical team comprised of real and virtual team members in a standard OR, incorporating real and virtual equipment. Scenarios in the VOR are described using a concurrent state machine methodology that supports non-linear scenario specifications with manageable complexity, even for heavily multi-branch scenarios. The main execution engine utilizes a flexible architecture that allows integration of external control signals that can affect scenario evolution. The paper describes the architecture and provides an example of a scenario addressing laparoscopic cholecystectomy moderated in real time through user voice recognition, instrument manipulation and hardware-based performance assessment. The voice recognition system utilizes a semantic interpretation grammar that allows detection of semantic responses even when spoken using different sentence patterns. A realistic physical simulation of an instrumented abdominal cavity is used to measure task-related performance. The voice recognition system and instrumented abdomen inform the virtual characters who can provide task-specific and summative feedback to the trainee. 