Consider a situation where multiple models are available to solve a certain modeling problem. Their performance varies on new data since they were developed based on different datasets by different modelers. One modeling approach is to select the most fitting model to some reference data through competition. Another approach, recently popular in challenges, is to combine the models into an ensemble model. Using combination, each model has a potential to influence the combined model and is superior to plain competition when possible since knowledge is accumulated rather than selected. There are several challenges in combination. One is determining the importance of each model. This challenge increases in difficulty where models are simulated using random methods such as Monte-Carlo. These simulations typically have accuracy related to time by square root order. So efficiency with respect to time and computing power required is a factor. This paper will discuss theoretical methods for combining models using very simple examples. Some implementations are discussed with reference to code examples. Advantages and drawbacks of the different techniques are discussed to allow modelers choose the preferred combination approach. Implications on accuracy and computing power and use of parallelization techniques are discussed. A disease modeling application using one of the techniques is briefly discussed. Modern job training often takes place away from the classroom. Students goals include  meeting training requirements, augmenting professional knowledge, and reaching career objectives. However, training opportunities are often limited by a students responsibilities and current tasking. Constraints also include the students location and schedule. In this work, we report on (1) a framework to formalize the representation, selection, and scheduling of training in a lifelong learning context. (2) An AI-scheduling algorithm to best choose training courses, venues, and schedules for the student. First, the framework includes training needs, including job requirements and career objectives. Second, the framework includes training opportunities, including information about training type, training location, competencies trained, and course schedules (times and dates). Third, the framework includes information about the trainee, including the trainees previous knowledge, skills, and experiences, and previous assessments. Once the framework is defined, sequential optimization algorithms can provide advice on training schedule. The algorithms consider hard constraints and soft constraints. Hard constraints include scheduling constraints and location constraints. Soft constraints include venue preferences, training environment preferences, and training type preferences. These constraints are balanced against student career objectives and goals. The selection is sequential, meaning that the algorithm looks ahead to the future, it is able to select a course because it is a prerequisite to another, later course. It is our hope that by carefully formalizing lifelong learning as an AI-scheduling problem, students will be able to achieve career goals and seize opportunities that balance against their existing responsibilities.            An Integrated Gate Turnaround Management (IGTM) prototype was developed at NASA Ames Simulation Laboratories (SimLabs) using Dallas Fort Worth International Airport (DFW) to demonstrate the IGTM concepts feasibility and benefits in improving the traffic performance during the turnaround at the gate over uncertainties from flight activities and available resources. The simulation architecture includes: the IGTM controller, an Airline Operations Control (AOC) application, Big Data/Analytics Input (BAI) application, a terminal traffic simulation or known as NASA-developed Surface Operation Simulator and Scheduler (SOSS), and a Database Server. ActiveMQ, a Java messaging service, was used to emulate the System Wide Information Management (SWIM) data network messaging. This paper describes the modeling and simulation of the IGTM concept, and illustrates selected use cases to demonstrate the feasibility and benefits of the IGTM concept for optimizing gate turnaround operations. There are many misconceptions about the benefits and application of pilot studies, but conducting tests without performing them in advance can be a costly mistake. One misconception is that access to test assets is necessary. Although it can be challenging to run a pilot study without the actual resources used in the test, simulation provides a means to overcome this and validate the effectiveness of the intended design. This paper presents the approach and results from a pilot study enabled through simulation. The project underlying this study is a test event investigating process performance. The team realized the value of incorporating a pilot study, but access to the systems and operators was not possible. They innovated a technique by combining Markov transition state modeling with process mining footprints to generate simulated data. Although simulated, its realism proved sufficient to inform a data collection plan optimized for a small team, and it indicated the main test was feasible. By analyzing the resulting data against investigative questions, the data collection strategy was refined in ways unforeseen before simulating the process. The pilot study clarified assumptions, refined instrumentation, and assessed the utility of expected data. These results demonstrate the power of pilot designs, even when resources are not available. This paper closes with insights extensible to other types of analyses and particularly those involving testing and process mining. Keywords: pilot study, domain expert, Markov model, simulation, process mining, process modeling, synthetic data The Mahalanobis Taguchi System (MTS) uses multivariate data in a unique way, to enable recognition, diagnosis, and prediction of important conditions for a wide variety of applications in various industries. The modeling and simulation discipline could benefit from adding MTS pattern recognition to their tool set for analysis of simulation results and decision support. MTS is based on the combined works of Dr. P.C. Mahalanobis, Dr. Genichi Taguchi, Dr. Rajesh Jugulum, Dr. Soichi Teshima, and others. Although the MTS approach is not well known in the U.S., it has provided benefits worth considering in many diverse applications over the past couple of decades, including manufacturing, healthcare, and vehicle control systems. It has proven to be superior in several ways to other recognition and prediction approaches, such as regression analysis and neural networks. Unlike more complex methods like artificial neural networks, MTS can be successfully demonstrated and experienced by analysts, programmers, and engineers with simple analysis tools like Excel. This technique may be useful to modeling and simulation researchers, and professionals interested in a straightforward recognition and prediction approach, which resolves multi-dimensional problems into a simple, single measurement scale. The paper will introduce 1) A concise explanation of the concepts behind the MTS and its various uses, 2) An introduction to the computational methods for MTS, including a simple live demonstration, 3) Optimization of the MTS approach to ensure reliable predictions with the fewest possible parameters, 4) Summary of several MTS case studies from various industries, and 5) Comparison of advantages and disadvantages between MTS and other recognition and prediction methods. Data Farming is a quantified approach that examines questions in large possibility spaces using modeling and simulation. It evaluates whole landscapes of outcomes to draw insights from outcome distributions and outliers. The Data Farming Support to NATO task group has codified the data farming methodology and a follow-on task group is now applying data farming to important NATO question areas. The development of data mining and data visualization techniques also continues to help us understand the huge amount of simulation data resulting from data farming. This paper outlines the documented data farming techniques, illustrates data farming in the context of quantitative analysis of cyber defense technologies and measures, and describes the links to predictive analyses made possible by advancing the connection of data farming to data mining and data visualization. The paper describes a prototype simulation using an agent-based model (NetLogo) that the NATO task group team has developed with feedback from subject-matter experts. The paper also describes the data farming the team has performed on key model parameters to begin to get insight into cyber security what-if questions. Army trainer maintenance tends to be based on a preventative maintenance schedule supplied by the trainer manufacturer and planned based on achieving standards of operational availability. These schedules do not take into account actual delivered usage. The Army can reduce costs by tying the maintenance of training systems to the use of the systems. Although there are many ways in which usage can be tracked, some combinations of metrics are more closely correlated to the need for corrective action maintenance. Selection of the most appropriate measure or set of measures often depends on the design of the training device. Generally this selection can best be determined based on historical data. Records of actual delivered usage are necessary because these values often differ from the expected usage. In order to reduce maintenance costs while improving training delivery, actual use of the training device needs to be collected. Data analytics is the scientific process of transforming data into insight for making better decisions and is used in industry to improve organizational decision-making and in the sciences to verify or disprove existing models or theories. Current data analytic models have begun to make an impact on the way that courses are designed, run, and evaluated, although little progress has been made towards the design of a structured method to categorize and implement data measurements as they relate to the Army Learning Model (ALM) goals. The following paper describes ongoing work with the U.S. Army Research Laboratory to examine data analytics as it relates to the design of courses, evaluation of individual and group performances, and the ability to tailor the learning experience to achieve optimal learning outcomes. This paper describes: a) the methodology for research and evaluation; b) the fields of Learning Analytics and Educational Data Mining; c) data analytics methods and techniques relevant to learning systems; and d) a framework for applying data analytic methods and techniques for learning via three illustrative use cases. Ultimately, the goal of this paper will be to provide a vision for successful application of these techniques within the Army learning community and higher education. Simulation modeling is a well-established and essential tool for epidemiologic investigations, including population disease dynamics and the evaluation of mechanisms for disease control. Agricultural economic models allow users to go beyond disease impacts to explore the cost implications of animal disease. The United States Department of Agricultures Center for Epidemiology and Animal Health uses a variety of modeling tools to answer critical questions for planning and policy. In addition to describing some of these modeling tools, this presentation also covers some lessons learned while supporting the 2015 avian influenza outbreak. 
Even a moderately sized real-life runway scheduling problem tends to be too complex to be solved by analytical methods where the proposed mathematical models for this problem all belong to the complexity class of NP-Hard in a strong sense. Therefore, it is only possible to solve practical runway scheduling problems using mathematical programming methods by making a large number of simplifications and assumptions. As a result, most of the analytical models proposed in the literature suffer from too much abstraction and, in turn, not much applicability in practice. However, simulation-based methods have the capability to characterize complex and stochastic real-life runway systems in detail, as well as cope with several constraints and multiple objectives, which are important factors in practice. With a simulation-based optimization (SbO) approach where a discrete event simulation model is integrated with an optimization algorithm, the search for Pareto-optimal solutions can be done conveniently. Due to its large and unstructured search space, finding exact Pareto-optimal solutions to such multi-objective optimization problem is computationally intractable; given that such solutions need to be found in a reasonable timeframe, metaheuristic algorithms are the best option to pursue. In this study, a hybrid metaheuristic algorithm based on scatter search (SS), which takes advantage of the structural details of the problem, and uses a dynamic update mechanism to produce high-quality solutions and a rebuilding strategy to promote diversity, is proposed and presented. SS-based multi-objective evolutionary algorithms seem to be a promising research direction due to its efficiency and effectiveness in finding a set of non-dominated solutions in a SbO framework with multiple objectives. The experimental results that evaluate the proposed hybrid metaheuristic algorithms performance in terms of both diversity of solutions and their proximity to the Pareto frontier are also presented. 