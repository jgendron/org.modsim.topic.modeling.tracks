Estimating extreme responses for ships and other marine structures is critical in the design process. Long-term simulations can, theoretically, capture extreme responses, but long-term simulations using robust, nonlinear computer programs are not computationally feasible in the early design process. An alternative is to use short, tailored simulations that create statistically equivalent extreme responses. This requires an accurate reconstruction of expected extreme sea conditions. Extreme responses can be estimated using a variety of processes, including the Design Loads Generator (DLG). Assuming a vessel response can be approximated as a sum of Fourier amplitudes with random phase angles, the DLG calculates sets of random phase angles for the seaway that result in a given extreme response or design event. The corresponding incident wave phase angles are calculated from the response phase angles and the incident wave train profile can be calculated. Depending on the response in question, the incident wave train profile from the DLG can range from a single, large wave to a group of 5-10 waves of similar frequency and amplitude. Accurately recreating these deterministic wave trains in a nonlinear seaway is a crucial step to integrating the DLG with a high fidelity CFD program. A common practice employs linear wave theory to shift the phase angles according to the location of the wavemaker. The shifted phase angles can then be used to drive a wavemaker with the intent of producing the desired wave train at the specified point downstream. However, nonlinearities in wave propagation result in inaccurately generated wave trains as measured at the specified point downstream. A recently developed method for the phase shift process allows for improved simulation of nonlinear water-wave evolution. This paper applies this method to increasingly large waves and assesses its performance in the limit of extremely large waves. Triangulation is a strategy for increasing credibility of research. It can be used to investigate the same phenomenon or system through comparison at different levels. The literature review covers state of the art of replication and comparison studies in Modeling and Simulation (MANDS). The following discussion identifies main differences between the terms triangulation, replication and scenario. Finally, different dimensions indicating possible configurations of triangulation are explored providing initial guidance on how they could improve credibility of MANDS research. While it is a supportable assertion that the conceptualization and realization of Forces Modeling and Simulation (FMS) strategies are well understood for kinetic warfare, the same assumption does not yet hold true for non-kinetic warfare, e.g. cyber warfare. The development of a strategy with respect to FMS cyber warfare is still evolving. To date, successful annual cyber exercises, such as Cyber Flag and Cyber Guard, offer promising introductions into the development of FMS tactics and strategy. The fact remains that these large-scale exercises cost millions of dollars to implement and support. However, with the expansion of open source tools and the enhancement of hardware services, such as High Performance Computing (HPC) configurations, a cost-effective and adaptive solution is not only desirable but also tenable. In response to this, we propose a Cyber Quick-Reaction Training Environment (CQRTE). The CQRTE concept is based on the philosophical tenets of McRavens highly regarded The Theory of Special Operations, which was an examination of eight important treatises on small warfare operation and strategy. This paper focuses on our Research and Development (R&D) efforts, which used HPC to stand up a low-cost fully operable cyberspace training and exercise environment. To the best of our knowledge, it is the first of its kind. The project has demonstrated how CQRTE can effectively model warfare principles within the context of cyberspace operations and, when combined, these principles can achieve relative superiority. The success of the envisioned CQRTE can serve as a guiding beacon for those combatant organizations whose mission-set requires continuous training and modeling, as well as the development of tools and tactics in the cyberspace domain. Person-Centered Studies (PCS) are a new approach for the design and analysis of medical and healthcare research with human participants. The PCS approach is based on the idea that data can be privately maintained by participants and never revealed to researchers, while still enabling medical and healthcare statistical models to be fit and research hypotheses to be tested. PCS rests on the assumption that data should belong to, be controlled by, and remain in the possession of participants in studies. Since data have value, individuals can accumulate personal wealth by participating in medical and healthcare science. The key observation behind the PCS approach is that medical and healthcare statistical models can be fit by sending an objective function and vector of parameters to each participants smartphone, where the likelihood of that participants data is calculated locally. Only the likelihood value is returned to the central optimizer. The optimizer aggregates likelihood values from all participants and chooses a new vector of parameters until the model converges. This transformative workflow relies on two modeling components: (1) a medical-data dropbox (MDB) for patients to maintain possession of their individual data and not reveal it and (2) a method for scattered likelihood estimation (ScaLE) so that each participants smartphone calculates the likelihood of their own data and passes only the likelihood value back to a centralized optimizer. The PCS approach solves or simplifies many current problems that plague medical and healthcare research. A PCS study provides: (1) significantly greater privacy for participants, (2) lower cost for the researcher and funding institute, (3) a larger base of participants and (4) faster determination of results. Interoperating Live, Virtual and Constructive (LVC) simulations has been major goal and challenge in Modeling and Simulation (MANDS) community. Achieving interoperability of LVC simulations is a technically and managerially complex task, and this makes simulation interoperability experts consider multiple factors originated from multiple domains. Successful interoperation of LVC Simulation is determined by the well-organized Systems Engineering (SE) process because SE process defines a generalized and overall process for building and executing distributed simulation environments. Thus interoperability readiness level of simulation systems and relevant organization can be measured as a part of the simulation interoperability design phase. This research aims to design a framework to measure the potential interoperability level of a simulation system and a relevant organization in technical, conceptual and organizational prospects. Specifically, an LVC simulation interoperability measurement framework that contains an LVC simulation interoperability maturity model and an interoperability measurement process is proposed. To accomplish the goal, a set of factors that determine the potential interoperability level of LVC simulations are identified through a literature review and a survey involving a number of relevant domain experts. The factors are used to build the key elements of the framework. A case study is demonstrated to prove the validity and effectiveness of the developed framework. Software development is a recursive process of development, testing, and revision; all of which take up time, money, and man power. For companies that have limited budgets and timelines for completion, simulation techniques can assist in software and hardware testing and verification. This paper focuses on the development of software using simulation and references the Riverscout case study as a means to show the effectiveness of using simulation during software development. The highest priority of successful testing is the need to replicate the behavior of the system within a virtual environment. This is accomplished through our SimArchitechture and comprehensive data collection and modeling techniques. Often times, testing hardware components that integrate through the software can be time consuming and deteriorate the hardware over time. In order to reduce time and cost, a simulation can be constructed to represent the hardware, where the simulation then becomes the test suite for the software being constructed. This paper shows how simulation aided the Riverscout projects software development process by reducing cost, development time, and deterioration to hardware used in the project. Finally results and conclusions are presented along with how expanding the use of simulation can be effective in other projects using software. Materials in medical simulations typically consist of polymers such as PVC, silicone, and some slightly more esoteric materials. While adequate for procedural training, they are typically lacking in the realism necessary to fully engage the learner. To improve the behaviors of simulated tissues, a better understanding of the mechanics of human tissue is required. To address this problem, the military is working closely with academia to broadly characterize fresh human cadaver tissues that are of interest to military medical learning. This paper compares the measured mechanical properties of simulated tissues from medical trainers against human tissues that would be subject to a chest tube insertion (skin, pleura). The research will also begin the development of models to translate human tissue data into performance requirements for future simulated tissues. There is a lack of interoperability, limited reuse and loose integration between the Live, Virtual and/or Constructive assets across multiple Standard Simulation Architectures (SSAs). There has been much research to solve these problems but their solutions resulted in complex and inflexible integration, long time of user-usage and high cost for LVC simulation. The purpose of this research is to provide an agile roadmap for the Live Virtual Constructive-Integrating Training Architecture (LVC-ITA) that will address the above problems and introduce interoperable LVC simulation. In addition, this research illustrated a case study using an Adaptive distributed parallel Simulation environment for Interoperable and reusable Model (AddSIM) that is a component based integrated simulation engine. The agile roadmap of the LVC-ITA that reflected the lessons learned from the case study will contribute to guide Modeling and Simulation (MANDS) developing communities to an efficient path to increase interoperability, composability and integration of LVC assets. During the past year the Defense Modeling & Simulation Coordination Office (DMSCO) has implemented major hardware, software and process upgrades to the discovery and reuse capabilities of the new Defense MANDS Catalog. Key enablers for the improved capabilities are the Enterprise Metacard Builder Resource (EMBR) Portal and the MANDS Community of Interest Discovery Metadata Specification (MSC-DMS), both DMSCO-developed, GOTS products that have evolved to complement and enhance the Defense MANDS Catalogs search and discovery features. This paper will demonstrate recent enhancements, describe a relevant case study and outline plans for future development.    Every systems engineer knows about the six interrogatives: who, what, where, when, how, why. Engineers and technical managers also know what knowledge about each system has to for each phase of its life cycle, and that in each phase multiple team members and stakeholders are involved. The likelihood for communication break-downs and misunderstandings is already great for just one system and increases manifold in complex systems that are part of a portfolio. Semantic technologies allow for a more consistent approach to capture the expert knowledge associated with systems across their life cycles and provide an economical approach to enforce consistency and enable flexibility in domain specific engineering efforts. The key elements and enablers that ensure expert knowledge is captured and consistently composed and represented are ontologies, and rule sets, and reasoning logic. Ontologies have been identified as successful support to ensure semantic consistency. This paper evaluates what ontologies can do to successfully address the multi-dimensionality of complex systems engineering challenges and makes some recommendations how a general solution may look like. Testing, evaluation, training, and characterization of large, complex information systems is difficult, expensive, and time consuming. Sandia National Laboratories has pursued research over the last 10 years to develop tools and technologies to help address these issues. Enter EmulyticsTM. The goals of this paper are to: (1) describe EmulyticsTM (emulation + analytics); (2) bring awareness to some of the current research efforts at Sandia and how they have been employed for a variety projects; and (3) share some lessons learned and success stories. The discussion of EmulyticsTM research begins by outlining the methodology for successfully creating models of large, complex systems using a variety of techniques. These systems include the blending of simulation, virtualization of hardware and software, emulation of devices, and direct deployment of actual hardware and software  that is, live-virtual-constructive (i.e., real-emulated-simulated) environments. Next, the discussion covers the building blocks used to create immersive, high-fidelity, system emulation environments to perform testing in areas such as: test and evaluation of security architectures and security devices; personnel training; course of action (COA) analysis; and application performance. Finally, we discuss some of the current research efforts, deployments and challenges. Effective weaponization of direct-fire remote and unmanned systems involves a complex interaction between warfighter and weapon system. To better understand the feasibility of such systems, a tele-operated and supervised autonomous (SA) remote weapon station simulator was used to conduct a series of warfighter experiments to validate the SA fire control methodology as a future system concept. Simulation and analysis are used to investigate the capabilities and limitations of SA, and benefits of SA as compared to tele-operation as a method of weapon system control. This paper presents the experiment design, modelling and simulation approach, and results of this study conducted with both civilians, and operational warfighters. Results indicate a normalization, and overall increase in measured operator performance in combat scenarios utilizing an ideal SA combat system as compared to ideal manual operation, however also indicate that peak performance of adept manual operators may exceed that of semi-automated performance. 