For high fidelity, Interoperation of heterogeneous levels of models could be suggested as one of the reasonable methodologies in defense modeling and simulation. Engagement-level model can be supported by engineering-level models representing the detailed functionality of its composites to increase the accuracy of Measures of Effectiveness (MOE). This paper presents a case study of anti-surface warfare (ASuW) simulation from the perspective of submarine to analyze MOE of Bearing-only Target Motion Analysis via heterogeneous models. Tactical decisions are built as engagement-level based on DEVS formalism and few physical behaviors of participants that have significant impact on the result such as maneuvering of submarine are built as engineering- level. By variant of tactics, thousands of experiments are conducted. Decision maker can select one among tactics that leads to high effectiveness. This case study shows that interoperation of heterogeneous models can serve as a sophisticated approach for evaluation of tactics with high fidelity. Computational population generation produces individual records using Monte-Carlo techniques. Generation rules for each individual characteristic potentially depend on other characteristics which create a dependency tree for computing each individual. In addition evolutionary computation objectives define selection criteria of individuals to match aggregate population goals. These elements repeat many times when creating new populations, therefore code reusability is important to simplify data entry. Moreover it allows exchanging computational building blocks to create simulation variations for sensitivity analysis or hypothesis testing which are essential for reference modeling. This paper describes an object oriented approach where population building blocks can be inherited to build a population that later can be rebuilt with variations. The paper describes the modular modeling theory with some basic examples. Analysts across the Intelligence Community (IC), including Department of Defense (DOD) intelligence components, face rapid growth in the volume and levels of fidelity of data to be transformed into intelligence. At the same time, the IC is experiencing a reduction of skilled senior analysts due to attrition and budget compression. At a time when all-source, multi-int, and open-source analysis are becoming critical to combating state-sponsored terrorism, transnational threats, and hostile proto-states, any knowledge, skills or experience gaps must be seriously examined and remedied. These challenges are familiar but new influences are exacerbating these gaps. Open source information has grown dramatically, driven by traditional news outlets offering digital editions and nontraditional conduits such as blogs and social media. Open-source intelligence (OSINT) has assumed greater importance as conflict zones are often in regions that lack traditional infrastructure and communications networks. In summary, analysts are facing unprecedented volumes of information and an acute requirement for advanced analytical techniques. A systemic improvement that could address these challenges is a new approach to training that embeds the effective use of proven, replicable methods for intelligence analysis in authentic scenarios. In this paper we describe a simulation enabled by online, scenario-based training incorporating a repeatable and proven analytic workflow. Tradecraft & Analysis Learning using Intelligence Scenarios with Methods-Anchoring (TALISMAN) applies scenario-based, technology-enriched training to create web-accessible interactive training scenarios. TALISMAN will equip analysts with the skills and knowledge to identify adversary locations, clandestine facilities, illicit activities, and other phenomenology using a proven composite signatures methodology. This study examined illegal fishing activity by Foreign Fishing Vessels (FFVs), which occurs to an unknown degree and with an unknown frequency. A discrete event simulation, coded in VBA within Excel for ease of use and distribution, was used to model FFV movements and behavior, Coast Guard presence, and probability of detection to estimate the likelihood of FFV discovery. Since system arrivals are a primary output in this case, instead of the standard input, we linked this likelihood of discovery through historically documented performance to estimate the number of system arrivals, or incursions, in a given year. Model data was derived from historical mission analysis, subject matter expertise, and recent academic studies. The resulting analysis regarding the extent of FFV incursions and the overall impact of this activity on both the fish population and the U.S. and foreign economies provided the first rigorous, quantitative estimates on the extent of this problem. Constructing and overhauling Navy aircraft carriers and submarines is among the most complex work undertaken anywhere in the world. An integral part of these operations is efficient material storage logistics. One of the largest challenges in developing logistical plans for material storage in this environment is the evolving nature of material needs; as ships are built the type of material they require to progress dramatically changes. In this paper, we introduce a simulation model which supports adaptive planning of material storage strategies. This tool utilizes discrete event simulation to adaptively plan warehouse storage layouts and parts turnover across the life of ship construction and overhaul. This tool allows NNS to accurately simulate the massive quantities of parts that arrive at the shipyard every month, route the parts based on optimal paths, and develop efficient storage strategies which are capable of growing and shrinking with program demand. Evacuations requiring large numbers of pedestrians to evacuate and potentially interact with vehicles are unique in the complexity of interactions that must be understood and in the potentially catastrophic consequences of poor management. On the pedestrian side alone, these events, which may result from a variety of causes including terrorist attacks, bomb threats, and building fires, require an understanding of individual and group social dynamics, a wide breadth of mobility levels, cultural tendencies, and many other factors. Past research, involving bimodal interactions, concentrate on intersections or pedestrian midblock crossings in nonemergency scenarios, but only rarely address panic-induced evacuations. Using an agent-based modeling approach, this study serves as a fundamental building block for establishing pedestrian behavior when navigating around vehicles during an evacuation. This model combines survey data with sociological and psychological factors obtained from the literature to capture the heterogeneous decision-making of the pedestrian evacuees. The model will include a range of evacuating agents and focus on the decisions of pedestrians interacting with a few vehicles to explore the basic bimodal dynamics that occur during emergency evacuations. Findings show that implementing diverse characteristics changes the pedestrian response in a panic-induced environment. This model lays the foundation for a fundamentally sound full-scale evacuation model. With the growth of social media, the value of text-based information continues to increase. It is difficult to analyze a large corpus of text to discover the structure within the data using computational methods. Alan Turing (1950) opens his influential article "Computing Machinery and Intelligence" with the statement, "I propose to consider the question, 'Can machines think?'" (p. 433). Overall, this Turing Test has become a basis of natural language processing. The essence of this project is to take a corpus of text and build a predictive model to present a user with a prediction of the next likely word based on their input. A key aspect of the paper is discussion of techniques balancing accuracy and scalability for large data sets. This paper provides the analysis decisions used to develop that predictive text model for a corpus of over 500,000 blog articles. The resultant model exists as a web-based data product that allows for customizable user reporting. Additionally, the work presented in this project follows the tenets of reproducible research and all code is available in an open-source repository to enable readers to review the approach, reproduce the results, and collaborate to enhance the model. Keywords: natural language processing, predictive model, text mining, predictive text analytics, N-Gram, data product, Good-Turing Smoothing, Katz back off Modeling under uncertainty has been of paramount importance, as quantitative methods of risk analysis have been developed to evaluate, prevent and respond to bioterror threats. Our simulation platform, Terrisk, improves risk assessment capabilities by providing modelers with novel means to: (1) capture incomplete and conflicting information about bioterror threats and (2) propagate that captured uncertainty to simulation outputs. Terrisk achieves these capabilities by supporting computation through three mathematical frameworks: probability theory, probability boxes, and Dempster-Shafer theory of evidence. The inclusion of the latter two frameworks (probability boxes and Dempster-Shafer theory) is novel. They offer alternatives to traditional probability theory, which enables incomplete and conflicting information about model inputs to be captured and propagated to outputs. This capability of Terrisk facilitates decision-making because it provides more insight into possible model outcomes by more accurately reflecting what is known about model inputs. The Department of Homeland Security (DHS) is interested in understanding capabilities to encounter people crossing the borders between official ports of entry, and in determining the likelihood and probable areas where law enforcement officials can intercept a suspect, either on or off road. Engility Corporation, under contract with DHS, has developed a general purpose geospatial encounter model as a plugin for QGIS. This model takes into account law enforcement and adversary vehicle types, road networks, natural boundaries, elevation, and land type to quickly and efficiently generate triangular meshes to compute isochrones accurately over entire regions. These isochrones support a physics constrained, quantitative assessment of potential encounter capabilities. Novel modeling approaches were introduced to improve computational efficiency of modeling multiple law enforcement and adversary entities in a single scenario. The software can also aid in planning future force structure laydowns to detect illegal border crossings and off road searches. Key Performance Indicators (KPI) are becoming an essential part of a manufacturing organizations ability to monitor and ensure its strategic health. Furthermore, selecting the right mix of KPIs, in line with an organizations strategic goals, is also essential. This paper outlines the development of a web-enabled database tool to support a selection method for KPIs for manufacturing which was developed by National Institute of Standards and Technology (NIST). In the method, KPIs are selected to effectively achieve certain criteria for a target manufacturing process. Subject matters experts use the tool to perform scoring of criteria weights and KPI/criteria pairs. These scores are analyzed and iterated in order to determine balanced KPI sets, which best satisfy the chosen criteria and the stated critical objectives. A prototype tool, a web-enabled database, was developed for the purpose of facilitating the pilot implementation of the NIST methodology at select manufacturing plants. This paper provides a description of the NIST methodology and the tool development. 