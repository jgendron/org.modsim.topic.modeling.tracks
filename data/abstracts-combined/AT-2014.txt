This paper describes a new simulation-based production planning and scheduling (PPS) system for use in managing a job shop manufacturing facility. A job consists of a process flow that contains all tasks that must be accomplished in order for the job to be complete. Each task can be performed in several workstations (resources) of varying performance. The order in which the task will be performed, as well as the resources that will be involved in the process is called a schedule and it is specified by the shop managers. However, when creating a new schedule, it is difficult to predict and account for the randomness in the system as well as the contention for facility resources that is likely to occur. The PPS system will accept a proposed job schedule and, by using simulation, will resolve the resource contention. It will produce a realizable job schedule that allows the user to see the impact of resource contention on the shop floor. This paper describes the high-level design of the PPS system, with an emphasis on the input and output sub-systems. Clinical trial population information is typically restricted and individual data is not public. However, clinical trial results and population statistics are regularly published. It is possible to reconstruct mock individual data populations from these statistics to support disease modeling and better understand the population characteristics. This can help in both planning and analysis of trial results on a larger information scope involving multiple clinical trials. A fairly simple example of generating an individual population from aggregate statistics is as follows: generate 1000 individuals such that their mean age would be 61 with SD of 8.2 and mean age at diagnosis of diabetes would be 53. Even this simple example has constraints such as age at diagnosis of diabetes should be lower than the individual age which will cause a skewed distribution. Reconstructing a mock population that matches clinical trial statistics is more complex and involves multiple objectives and interactions between statistics. This work improves the Monte Carlo abilities of the MIcro Simulation Tool (MIST) to generate populations from statistics by introducing genetic algorithms supported by the INSPYRED software package. The genetic algorithm improves the accuracy of the reconstructed population and better handles skewed distributions and constraints. MIST and INSPYRED are both free software available under GPL license and can be downloaded through these links: https://github.com/Jacob-Barhak/MIST https://github.com/inspyred/inspyred Lifecycle cost analysis and Return-on-Investment (ROI) forecasts, are important processes associated with systems engineering and its sub-discipline, Human-Systems Integration (HSI). The Office of the Secretary of Defense (and many other Defense organizations) mandate the use of HSI, and analysis and acquisition efforts that apply HSI practices are more likely to be successful (e.g., Pew et al., 2007). Our team is presently investigating best practices for applying such evaluation techniques to medical training simulation systems as part of a larger effort focused on the U.S. Army. In this paper, we will share our generalizable best practices that developers and project engineers should consider in their own system initiatives. We offer these suggestions in order to encourage fellow medical simulation researchers and developers to use similar analysis and reporting methodologies, which will ultimately enhance comparability and clarity across the military and medical simulation-based training community. Many professionals are involved in the creation of Live, Virtual, Constructive, and Game (LVC&G) simulation environments and scenarios, from simulator programmers to subject matter experts. Currently, there is no systematic method for designing simulation events from these multiple perspectives. To meet this need, Aptima developed LVC&GAED (Assisted Experimental Designer), a decision support system that guides individuals through a ten step simulation-design process, from defining the research question or training goal, to choosing variables of interest and developing relevant measures. A knowledge database, populated by data from past simulations and their results, serves as the basis for an underlying model that recommends simulation configurations that address all relevant goals, while maximizing quality and minimizing cost. This prototype was designed to be usable by any individual, regardless of background, enabling them to create effective simulation designs quickly and efficiently. By utilizing all existing data and available tools, LVC&G-AED saves manpower, time, and financial resources. Resilience is an adaptive approach for managing a wide-range of energy/infrastructure-related risks. Simply stated, resilient entities survive and thrive under changing conditions. In contrast with traditional risk management approaches, which address discrete events or threats, resilience is a proactive, holistic approach to structuring fundamental processes and relationships to make them robust but flexible to respond to any type of challenge or change. Moreover, resiliency objectives focus on outcomes, such as protecting life and property or achieving a mission, in the face of change, rather than on simply protecting systems designed for nominal performance. Resilient systems may retain health through passive features, operational flexibility and decentralized decisions, which are more likely to manifest through iterative, even experimental processes, rather than traditional linear-design processes. In the context of communities and infrastructure, most traditional analysis techniques have focused upon preservation of essential functions, with some attention given to physical system interdependencies such as reliance upon power supplies. Considering the fundamental contrasts between deterministic design optimization concepts and the broader consideration of complex, cross-domain relationships in resilience thinking, there is an evident need for new models and techniques. Two relevant but contrasting approaches offer useful examples to explore in the context of this challenge. Military capability development procedures exemplify structured analysis toward a complex outcome, integrating qualitative operational analysis (war-gaming) with quantitative system (engineering) models in a correlated manner. Civilian emergency response communities focus on more flexible, descriptive processes that examine operational scenarios and gather expert insights which in turn inform stakeholder action, such as electric utility investments to improve reliability. This paper compares these techniques and offers recommendations for a hybrid approach to resilient design and energy portfolio development. The maintenance and overhaul worked performed on US Navy air craft carriers may be the most challenging industrial and engineering task undertaken anywhere by any organization (RAND National Defense Research Institute Study, 2002). During this complex process, sets of specialized tanks can cause unforeseen ripple effects on the entire overhaul process. In this paper, we introduce the utilization of MANDS to create a tool for adaptive planning of specialized tank inspection and overhaul. This tool utilizes discrete event simulation, implemented in NNSs own simulation engine, to replicate the complex job processes involved in this work. This tool allows NNS to support work by providing an adaptive planning capability, accepting data about performance to date and providing likely scenario outcomes. Additionally, the tool supports pre-planning for future carrier work in these specialized tanks, taking previous performance measures and using them as a baseline to assess proposed new work methods. Copyright 2014 Huntington Ingalls Industries, Inc. All Rights Reserved The Homeland Security Portfolio Value Model was developed by Old Dominion Universitys (ODU) Virginia Modeling, Analysis and Simulation Center (VMASC) under the direction and guidance of the Virginia Department of Emergency Management (VDEM) and the Office of Veterans Affairs and Homeland Security in 2012 and again in 2013. The model was developed to aid senior executive decision makers in funding allocations for the Virginia Homeland Security Grant Program (HSGP). This paper provides the background for the project, the decision making environment, and modeling objectives. Then the model development process is described in which researchers elicited from senior leadership the scoring criteria, weighting, and value functions to be used for project scoring. This is followed by a description of the database development and deployment used to capture data and administer the proposal scoring process. The paper then provides a summary of the scoring results as well as allocation decisions, conclusions, limitations and future work. Abstract: Organizations today are under increased pressure to respond to rapidly changing conditions. Managers are faced with having to make complex, expensive decisions, riddled with risk and uncertainty. This paper presents results where decision analysis helped organizations make better-informed decisions, faster. Through the use of collaboration, mathematical, and organizational behavior tools this paper presents several technologies that add rigor to the decision support process through an emphasis on refining the objective, finding evidence, analysis, visualization and a taking action framework. From heuristics and optimization to simulation and predictive models, this paper shows where computer based techniques provided traceable, repeatable methodologies that assisted organizations in decision support. This paper provides empirical and parametric evidence that show how modeling and simulation can provide faster, more accurate reporting, improved decision making, improved customer service and reduced costs. Simulation-Based Training (SBT) provides safe settings for training tasks that otherwise are too challenging or risky in live training counterparts. SBT also affords the development of controlled environments for user acclimation with emerging technologies, such as Unmanned Ground Systems (UGS). The military emerging technologies literature demands an increase in robot-aided Intelligence, Surveillance, Reconnaissance (ISR) for the detection of High Value Individuals (HVI) and identifies SBT as a required medium for UGS operator training. Robot-aided ISR is a predominantly perceptual task. Prior experimentation suggests that Highlighting and Massed Exposure instructional strategies enhance perceptual skills training by reducing trainees response time for overall behavior cue analysis a critical skill for HVI identification. The objective of this experiment was to assess the impact of these strategies on detection accuracy and speed for each cue type which indicate nervous or aggressive behavior. Results suggest that Massed Exposure improves response time for detecting nervous human behavior cues. Translating results from laboratory-based research studies conducted with novice participants (e.g. university students) to real-world applications represents a critical challenge facing researchers. Risk mitigation cannot wait until data is collected and analyzed, rather it must permeate every phase of the research process. Empirical evidence suggests that prudent application of fundamental human factors and training principles support experimental findings that equate to relevant recommendations for expert populations regardless of sample population experience. This paper presents five compelling strategies for conducting human participant research using novice populations that facilitate empirically sound insights for expert operators. Specifically, (1) designing experiments, (2) distilling skills into core components, (3) scaffolding, (4) proficiency testing, and (5) interpreting results will be discussed. The methods described represent the best practices in ongoing research efforts impacting highly specialized expert populations: Warfighters and nuclear power plant operators. The recommendations provided illustrate the potential that interdisciplinary experimental methods offer quantitative researchers. 