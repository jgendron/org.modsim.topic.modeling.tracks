For domains where data are difficult to obtain due to human or resource limitations, an emphasis is needed to efficiently explore the dimensions of information spaces to acquire any given response of interest. Many disciplines are still making the transition from brute force, dense, full factorial exploration of their information spaces to a more efficient design of experiments approach; the latter being in use successfully for many decades in agricultural and automotive applications. Although this transition is still incomplete, groundwork must be laid for incorporating the next generation of algorithms to adaptively explore the information space in response to data collected, as well as any resulting empirical models (i.e., metamodels). The methodology in the present work was to compare metamodel quality using a fixed sampling technique compared to an adaptive sampling technique based on metamodel variance. In order to quantify metamodeling errors, a delta method was used to provide quantitative model variance estimates. The present methodology was applied to a design space with an air-breathing engine performance response. It was shown that competitive metamodel quality with lower associated error could be achieved for an adaptive sampling technique for the same level of effort as a fixed, a priori sampling technique. A review of Decision Support Systems literature finds that such systems have historically been associated with managerial or industry long-term decision-making (Alter,1980). Decision Support Systems also refers to an academic field of research that involves designing and studying systems in their context of use (Schuffetal. 2011). This paper discusses the extension of the field to the support of decision-making for Combatting Weapons of Mass Destruction (CWMD). It describes the development of a scenario-based Decision Support Simulator (DSS) prototype using an iterative design approach that leverages a working group of subject matter experts to identify simulator and scenario requirements. The goal for the DSS is to enable decision makers to develop courses of action in response to crisis events by simulating response cells, logistics information, doctrine tactics, and procedures in a real-world context. At critical decision points during DSS scenarios, direct feedback and metacognitive prompting are presented as appropriate and key performance metrics are recorded for comprehensive after action review. Existing operational tools are leveraged to facilitate realistic scenario interactions. The goal is to unify disparate technologies and resources through a web interface that is extensible to multiple areas of expertise when dealing with crisis events. This paper details the approach to establishing the requirements of the design of a portable DSS prototype, including the CWMD scenario, instructional and system architecture, and assessment methodologies. Abundance of information is collected in the Intensive Care Unit (ICU) during patient stay. To help humans cope with abundance of such data, computers can help with 1) pre-processing and 2) visualization. Fortunately modern languages such as Python provide good tools to help programmers aid in those tasks. Tools such as Pandas allow import of data into a standard dataframe that can be processed with machine learning algorithms using scikit-learn, and later this data can be visualized interactively in web environment using Bokeh. The Jupyter notebook interface allows quick prototyping of the methods. This work shows how this collection of tools can be used to visualize patient data and filter it for outliers using multiple machine learning algorithms. The prototype created in this work is applied on a public ICU dataset with the intention of future extension towards automated detection of outliers in the data. Such outliers can either indicate hazards or should be removed before further analysis. Open source code and visualization are publicly available to support this paper. Big data is a proven asset within the hospitality industry  where might modeling and simulation further increase the value of data housed in hospitality data systems? Many industries find insights from big data and act on them to realize benefits. The hospitality industry uses data to focus marketing campaigns, enhance customer experience, optimize internal operations, and increase operating profits. This paper provides an overview of how hospitality sectors are using big data in a high stakes business. Restaurants represent a large sector of the hospitality industry. Restaurateurs manage operations ranging from multi-kitchen, four-star hotels to locally-owned restaurants. Trends in the restaurant sector show increased use of technologies to manage those operations. Mindful of the technologies restaurateurs use, this paper provides a look at big data within the context of these technologies and how predictive analytics is a leading capability for analyzing information. It highlights challenges restaurateurs faced in becoming data centric and viewing the switching costs as an investment. As data-centric organizations mature, shifting from predictive analytics to prescriptive analytics is a natural step. Modeling and simulation is a powerful approach to push beyond prediction and provide restaurateurs with insights they can act upon. Like big data, modeling and simulation solutions will need to overcome challenges before restaurateurs adopt them. This paper closes with three overarching questions to begin the conversation with restaurateurs and stimulate ways in which simulation professionals may provide answers. Keywords: big data, hospitality industry, restaurant sector, data science, data maturity model, customer intimacy, social media, cloud-based services, predictive analytics, prescriptive analytics, restaurant technology stack Data Farming is a methodology that combines simulation modeling, rapid scenario generation, high performance computing, experimental design, analysis and visualization, and collaborative processes. Data farming is a quantified approach that examines questions in large possibility spaces and evaluates whole landscapes of outcomes to draw insights from outcome distributions and outliers. Wargaming is an activity not involving actual forces, but represents military operations in conflict situations of various types. Wargaming normally attempts to adjudicate sequential player decisions within a representation of simultaneity. Each player, who can represent a nation-state or other political-military entity, takes actions in turn based on the current situation and within the limits of the game as to their ability to react to the most recent moves made by players before them during a game turn. In this paper we describe our work combining the power of data farming with wargaming in order to demonstrate the synergies between the two in capturing the dynamic interaction among friendly, adversary, and other forces. We discuss our work with a Hybrid Warfare scenario involving coalition forces requiring extensive knowledge of the decision-makers and their long-term goals, their risk attitude, their rational and irrational constraints, diplomatic capabilities, and strategic agility. In particular, we consider the elements of conventional forces and irregular forces; police forces and influence on urban/rural areas; force mix of political, military, industrial, and cyber warfare initiatives; national threat influence from beyond borders, election interference and external propaganda effects; and impacts of cyber and national infrastructure interference.  Modeling and Simulation techniques have evolved tremendously over the years. But a significant gap remains between the creation of analytical insights and delivering effective, actionable information to non-analytics enabled stakeholders. The last mile in analytics and decision making is language. Whether you have prescriptive or predictive analytics abilities, embraced Big Data, and have state-of-the-art analytics tools that process data in real time, you still need to explain your results, including the reasoning process, in a language the reader understands. No one ever made a decision because of a number. They need a story. In January 2017, Forbes called Natural Language Generation (NLG) one of the hottest trends of the year. Gartner has described NLG as the last-step in the Data Discovery and BI & Analytics processes. Essentially, Natural Language Generation is software that knows how to write automatically. It connects to data and Big Data tools and explains the results of the analysis in plain English or other local language. You no longer have to settle with static structured statistical analysis results reports. With NLG, executive summaries and detailed analysis reports that discuss performance can be automatically written with drill down explanations of trends and outliers. NLG solutions can be integrated with interactive dashboards where data can be selected and the descriptive insights focused on this selected data can be automatically updated. Food security and sustainable agriculture are two of the challenges faced by nations globally. As a population grows, the demand for food rises. To keep up with the demand without compromising the environment, sustainable agriculture techniques are significantly being studied and advocated by concerned organizations like the United Nations (UN). The UN furthers sustainable agriculture through its Sustainable Development Goal 2 (SDG 2) which aspires to double the agricultural productivity and incomes of small-scale food producers and ensure sustainable food production systems and implement resilient agricultural practices. Smallholder farming households, which has an estimated global population of 500 million (around 2 billion people), rely on small-scale agriculture for their livelihoods. They are considered as the backbone of agricultural production in developing countries and they play a key role in upholding sustainability. Having a crop rotation sustainability assessment tool for smallholder farmers can aid them accordingly in their crop production planning and abet the advocacy of agriculture sustainability. Our research aims to develop a model-driven decision support tool for smallholder farmers to promote sustainability in their crop production practices. In this paper, we investigate the integration of crop simulation model and multi- criteria decision analysis as an approach for a dynamic and multi-criteria sustainability assessment of crop rotation alternatives. The Exploration Medical Capability (ExMC) element at NASA Langley Research Center desires a process for evaluating and comparing designs for medical workstations capable of supporting long-duration human space exploration . The authors, as part of their senior capstone design experience, are jointly charged with both designing an evaluation process and implementing a prototype of that process as a proof of concept. A key component of the evaluation process is a simulation having the capability to assign quantitative values to dynamic performance measures. Dynamic performance measures are evaluated by observing the conduct of an activity or task over a period of time. Examples of dynamic performance measures include the time required to complete a task, or the volume/shape of the spatial envelope required to perform a task. The design team has determined that using a discrete event simulation, in which avatars are monitored while performing representative medical tasks, is the best approach for evaluating dynamic performance measures. The focus of this paper is the process of defining those metrics, as well as the approach to evaluating the dynamic metrics of the corresponding simulation. Stochastic animal disease models produce large quantities of data outputs regarding the epidemiological and economic impacts of disease outbreaks across a range of outbreak possibilities. The amount and complexity of data outputs can increase exponentially as more iterations and scenarios are performed to address the modeling question(s) of interest. Once analyzed, simulated output data can be valuable to decision makers for assessing the effects of different control strategies on outbreak impact and informing policy. In order for animal disease spread models to be useful tools to support emergency preparedness activities, the model outputs must be effectively provided to decision makers in a timely fashion. Typically, the process of obtaining the model output data and its analysis can be the rate limiting step that prevents model output from being readily used to inform decision making, as this process is dependent on the analytical skill sets of the modeler and access to appropriate software. We developed a suite of epidemiologic analyses and interactive model output visualization and scenario comparison tools using the R program (R Core Team, 2017) that link seamlessly to the outputs of the Animal Disease Spread Model (ADSM), a freely available modeling tool that can support emergency preparedness activities. A demonstration of the developed analysis and visualization R tools will be presented, employing a use-case of how they can be applied by a modeler to evaluate control strategies for a hypothetical foot-and-mouth disease outbreak and present a summary and comparison of the model scenarios to a decision maker. The acquisition system is broken.It is an axiom that the acquisition system is built for an industrial-age world and has demonstrated its inability to keep up with the demands of a post-industrial world. It is taken on faith that reforming the acquisition system should focus on the acquisition workforce. Addressing the root causes of perceived ineffectiveness of the acquisition system includes many people and processes that are generally not considered in acquisition reform. The latest solution to address symptoms rather than root causes is the Cross Functional Team, which is nothing new; it is the concept of the overarching integrated product team (OIPT)  already part of acquisition doctrine  by another name. It is time to consider the entire acquisition system, leveraging modeling tools and not just program execution. This paper begins the discussion of five areas that the authors believe can improve acquisition that are not being addressed in current discussions: the requirements process, the process of selecting PMs, disincentives to innovate, the budgeting process, and use of Government-owned simulation tools. We model the evolution of a complex technology in a specific geographical region by treating it as a living organism evolving genetically in an ecosystem. A simulation of the model is carried out by data-mining all patents issued in that region to inventions using that specific technology. Each patent provides a set of genes from a possible genome of the organism, with the genes being specific technological areas that the patent office uses to classify the functionalities of an invention. The results of the analysis can be used for decision support and business intelligence by providing a method of genetically engineering a technology to make it more fit for its local environment. Using the model, we examine how the genetic makeup of an organism changes over time as specific genes are selected by its environment. The genes collected from the patents provide a picture of how the species is evolving. The present simulation studies the evolution of Modeling and Simulation (MANDS) itself, with the state of Virginia as the local environment. In previous studies using the model, it was shown that environmental factors influence the species genome over time to generate organisms with specific capabilities, with MANDS in Michigan supporting the automotive industry and MANDS in Texas supporting the energy sector. Thus, the model leverages big data to understand how a complex technology evolves, how the regional environment provides supportive conditions for gene expression, and how to genetically modify the complex technology for future survival and growth. A submarine hull join is the process of fitting and welding two large hull sections of a submarine into a single unit. Until now, the ability to predict dimensional shifts from initial cylinder fit-up through the final weld relied on the domain expertise of experienced shipbuilders. With an infinitesimal tolerance for each control point, the care put into these determinations was tedious and time-consuming. To improve efficiency, a predictive algorithm was built to learn optimal at-set coordinates from the behavior of prior joins. The algorithm took historical data from a series of Virginia Class Submarine joins, and used regression and statistical analysis to model control point shifts at each stage. Predictions were generated for the shifts in a Monte Carlo Simulation, and an Excel Solver was used to choose the at-set coordinates that minimized the probability of out-of-tolerance joins. The VCS C-Seam Analytics Platform (VCAP), written in R, employs graph theory and advanced statistical distribution fitting rules to fully automate this algorithm. The mathematics are hidden behind an intuitive user interface, passing the burden of modeling the data to the tool instead of the engineer. The platform greatly reduces the man hours needed to prepare for each join, enables speedy enhancements to the models predictive power, and births an algorithm that progressively gets smarter from both new data and the users ability to quickly tune its parameters. 