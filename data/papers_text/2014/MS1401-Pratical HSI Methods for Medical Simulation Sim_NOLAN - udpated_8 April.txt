Practical Human-Systems Integration Methods for Medical Simulation Lifecycle Costing
Patricia Bockelman, Danielle Julian, Margaret Nolan MESH Solutions, LLC, a DSCI® Company Orlando, Florida pmorrow@mesh.dsci.com,djulian@mesh.dsci.com,
mnolan@mesh.dsci.com
ABSTRACT
Lifecycle cost analysis and Return-on-Investment (ROI) forecasts, are important processes associated with systems engineering and its sub-discipline, Human-Systems Integration (HSI). The Office of the Secretary of Defense (and many other Defense organizations) mandate the use of HSI, and analysis and acquisition efforts that apply HSI practices are more likely to be successful (e.g., Pew et al., 2007).
Our team is presently investigating best practices for applying such evaluation techniques to medical training simulation systems as part of a larger effort focused on the U.S. Army. In this paper, we will share our generalizable best practices that developers and project engineers should consider in their own system initiatives. We offer these suggestions in order to encourage fellow medical simulation researchers and developers to use similar analysis and reporting methodologies, which will ultimately enhance comparability and clarity across the military and medical simulation-based training community.
ABOUT THE AUTHORS
Dr. Patricia Bockelman is a Human Systems Research Scientist at MESH Solutions, LLC, a DSCI® Company. Her background includes research in cognitive science, intelligent behavior, learning and training, and simulation environments. Her expertise includes applying interdisciplinary techniques to combine neurophysiological, psychological, and phenomenological metrics to evaluate human experience. She holds a Master of Science degree in Modeling and Simulation with a Human Systems focus, a Master certificate in Cognitive Science and a Doctorate in Modeling and Simulation from the University of Central Florida.
Danielle Julian is a Human Systems Research Scientist at MESH Solutions, LLC, a DSCI ® Company. Her background includes research in Human Factors and learning and training. She holds a Bachelor’s degree in Psychology and is currently pursuing a Master of Science in Modeling and Simulation at UCF.
Margaret Nolan is a Senior Performance Engineer and Program Manager at MESH Solutions, LLC, a DSCI ® Company. She has over 25 years of experience in providing Human Factors, Cognitive Psychology, and Human Systems Integration (HSI) expertise, and project management support for numerous USMC, Navy, and Army training and simulation systems from development to final delivery as well for as Research, Development, Test & Evaluation efforts (RDT&E). She is certified in and has applied experience with implementing acquisition processes, ROI, and sustainment efforts.
Dr. Teresita Sotomayor is a chief engineer and subject matter expert in the area of severe trauma simulation at the U.S. Army Research Laboratory (ARL), Human Research and Engineering Directorate (HRED), Simulation and Training Technology Center (STTC). Her expertise in user-centric design and technology effectiveness evaluations has been instrumental in the development and transition of modeling and simulation solutions in support of medical training. She is a graduate of the University of Puerto Rico (Mayaguez Campus) with a degree in Industrial Engineering. She holds a Master of Science degree in Operations Research Stochastic Simulation from The George Washington University and a Doctorate in Modeling and Simulation from the University of Central Florida. She is a member of the Army Acquisition Corps since 2003 and has over 25 years of experience in the modeling, simulation, and training domain.
2014 Paper No. MS 1401 Page 1 of 9
Teresita Sotomayor ARL-HRED STTC Orlando, Florida teresita.sotomayor@us.army.mil
MODSIM World 2014

Practical Human-Systems Integration Methods for Medical Simulation Lifecycle Costing
Patricia Bockelman, Danielle Julian, Margaret Nolan MESH Solutions, LLC, a DSCI® Company Orlando, Florida pmorrow@mesh.dsci.com,djulian@mesh.dsci.com,
mnolan@mesh.dsci.com INTRODUCTION
Teresita Sotomayor ARL-HRED STTC Orlando, Florida teresita.sotomayor@us.army.mil
Simulation-based training (SBT) technologies have expanded as a training technique, precisely because they can offer hands-on practice while expediting cognitive readiness for transfer into the real world. SBT technologies improve trainees’ knowledge, skills, and attitudes (KSAs) (Gaba, 2004), but as helpful as these tools are, there are always caveats. For example, it is imperative that we ensure that medical SBT methods continue to address existing training gaps, inefficiencies, and offer current opportunities to support skill competency development and improve training effectiveness associated with military medical personnel.
Our team is presently investigating best practices and gaps in medical simulation training systems as part of a larger effort focused on the U.S. Army. Because of the ubiquitous applications of SBT across all forms of both military and medical training, it is imperative to systematically evaluate their use and efficacies relevant to specific domains. While there are various ways to approach these evaluations, some are more appropriate for the complex and dynamic systems that include human-in-the-loop and technology. Human-System Integration (HSI) is a philosophical and technical approach suitable for evaluating SBTs in all stages of application. HSI-informed Front End Analysis (FEAs) of SBT techniques in military medical contexts directly apply to best practices and are leveraged to support future medical training decision-making for Army Nurses. In this paper, we will describe how an FEA helped identify gaps and articulate best practices for a previous military medical study on Army Nurses and briefly discuss our future Research and Development (R&D) effort for Army Combat Medics.
HSI-INFORMED FRONT-END ANALYSIS
HSI, as a term, has been applied broadly and across domains with nuanced variation. It is beyond the scope of the present work to capture the breadth of the terminological application, but it is necessary to acknowledge that across and between domains, HSI may be used to refer to somewhat different things. Therefore, the exploration of the present topic begins by explicitly establishing our intended application of the term.
The present application of HSI speaks to the philosophical approach to understanding, designing, and improving systems by considering the human component at every level. HSI methods and techniques drive regulation, acquisition, design, manufacturing, and operation of technologies in a manner that aims to shape complex systemic relationships (Booher, 2003). Taking the human element seriously, a range of industries have embraced HSI to create safer, more reliable, and cost effective systems (Schmorrow & Nicholson, 2012). Beyond the private sector, HSI concepts have been formalized into Department of Defense policies (“FY09 Department of Defense Human Systems Integration Management Plan, Version 1.0,” 2009) and it is from this vantage that we consider HSI. That is, HSI is both the theory and the practice of augmenting design by intentionally shaping the interactions between the humans and the technologies in a system (Fass, 2006).
Consequently, the interactive components of HSI apply to a range of system analyses. The present work specifically focuses on a HSI-informed FEA. As FEAs can involve many possible subcategories of analyses, the FEA discussed herein paid particular attention to the following critical factors in training situations: knowledge, skills, and attitudes (KSAs) of targeted learners, user perceptions of system interactions, and efficacy of instructional delivery methods. This upfront attention to understanding existing gaps in training is key to supporting decision-making in a manner that increases Return-on-Investment (ROI) by preparing for down-the-road uncertainties (i.e., economic, environmental, and technological uncertainties, cf. Achiche, Appio, McAloone, & Di Minin, 2013). When
2014 Paper No. MS 1401 Page 2 of 9
MODSIM World 2014

researchers and instructional designers approach an FEA from a distinctly HSI perspective, that analytical process will include attention of difficult-to-assess aspects of the human within a given system, such as possible apprehensions, values, and perceptions of the stakeholders (Rouse, 2007).
Designing an FEA appropriate for military medical training requires consideration of a variety of stakeholders, including the trainers, trainees, the broader medical community, and the public as a whole. One way to approach such a wide-reaching stakeholder base might be to conduct large-scale examinations of each trainee, instructor, and subsequent medical care recipient, but this is far from reasonable. Therefore, the FEA conducted herein targets a distinct population and subject-matter experts (SMEs) within that population, coupled with expert observation of training facilities and methods to generate recommendations principles and techniques for future medical simulation training that would apply to the population examined and, we will argue, the medical community at-large. It is accomplished by tightly controlling the tools and techniques used in evaluation and extrapolating generalizable insights.
Those tools and techniques, in the present case, focus on the training specialized areas of military medicine. Within that context, a FEA can hone in on specific techniques, such as the implementation of simulation-based training (SBT). The present work drew from a range of analytic models for addressing human-system performance (cf. Harless, 1978; Mager & Pipe, 1997); the resulting model accommodated the constraints of military medicine within a framework broad enough to adequately address the currently ample concerns in the state-of-training. Of paramount importance in developing a practical field model for the HSI-informed FEA was that it could capture the iterative nature of training, with the flexibility for accommodation of new technologies and practices and the cyclical aspects of formative instruction. Our approach (see
Table 1) accomplished this, allowing us to test the FEA in a specific context (i.e. a proof-of-concept study in Army Nursing), and focus on particular techniques (i.e. the role of simulators in training).
Table 1. HSI-Informed FEA for Evaluating Military Medical Training.
MODSIM World 2014
    Five phases
        Driving questions Primary data collection sources
      Analysis What are the training end-goals? What Literature, expert interview performances do we want to effect?
       Design What are the strategies for instruction? Observation, expert interview, survey
       Development What steps are used to update training to Expert interview, literature address new concerns/best practices?
       Implementation What actually happens in the training? Observation, expert interview, survey
       Evaluation Did the training do what it intended to? Expert interview, literature
 From there, researchers, instructional designers, and other stakeholders can consider how training techniques, as captured by a HSI-informed FEA, might apply more broadly to best practices in medical training. The following sections unfold with that in mind; that is, the present work provides an overview of a study in the field of Army Nursing and leverages the results to inform future work in another military medical training context, Army Combat Medics.
TRAINING ARMY NURSES
The Army Nursing specialization acts as an essential part of military medical readiness, however, it has proven a difficult niche for medical training. It faces all of the same challenges inherent to any other nursing training context: there are gaps between the expectations of the classroom and the realities of practice (Corlett, Palfreyman, Staines,
2014 Paper No. MS 1401 Page 3 of 9

& Marr, 2003). The theories essential for grounding higher-level skills do not always translate beyond rote memorization in a manner that allows applied practice (Rolfe, 1996). The programs to transition nurses from the classroom and into practice vary widely, resulting in a range of experiences with unpredictable levels of support from mentors and preceptors (Rush, Adamack, Gordon, Lilly, & Janke, 2013), consequently, inadequate transition leads to performance decrements and job dissatisfaction (Jimenez, Navia-Osorio, & Diaz, 2010; Sharif & Masoumi, 2005). Military nursing combines the challenges characteristic of civilian nursing with difficulties unique to the care of armed services. The stressors of traditional nursing are amplified when nurses are deployed to hostile locations, must maintain Soldiering skills, and regularly are exposed to the medical challenges of combat-related injury. A meta-analysis of nursing stress supports this assertion, the military nurses examined showed a greater inverse correlation between job satisfaction and stress than civilian nurses and they reported significantly more stress and lower working relations (colleague interaction) satisfaction than their civilian counterparts (Khamisa, Peltzer, & Oldenburg, 2013). The complexity of military nursing adds demands for psychological resilience, as they must simultaneously care for the whole person of their service member patients while also retaining wellness functionally, emotionally, and cognitively to execute the jobs for which they have been trained (Simmons & Yoder, 2013).
Simulation-based Training for Army Nurses
One of the ways that nursing (and the medical profession at large) has addressed demands from complexity and transition is to leverage various forms of simulation in training. Simulations accommodate practice at various degrees of fidelity to support cognitive readiness, as well as psychomotor skills competency, in the field. The prevalence of simulation in medical training is so great, that it is not a question of whether a training facility uses some form of simulation, but rather which form it uses. The efficacy of simulation-based nursing training and evaluation has been empirically affirmed (for an overview of the supporting literature see Schatz, Marraffino, Allen, & Tanaka, 2013). Of particular value in the military context, where nurses may be working with extreme constraints, is simulation’s capacity for improving critical thinking and on-the-job confidence (Gordon, Oriol, & Cooper, 2004; Wilford & Doyle, 2006).
As Schatz, et al. highlight, SBT can present obstacles. They identify instructor and faculty apprehensions, including lack of strategic implementation for instruction (cf. Johannsson, Ayida, & Sadler, 2005; Muramoto, Campbell, & Salazar, 2003) and potential lack of standardization in implementation and measurement. Instructors and students have expressed concern over learning transfer, wondering whether issues of artificiality during training may introduce problems later during actual practice (Barach, Satish, & Streufert, 2001). Such obstacles, while they merit deliberate attention, are not insurmountable (Salas & Burke, 2002; Schatz et al., 2013), but they should be approached from an authentically HSI perspective.
The unique Army Nursing population, with its range of intrinsic and extrinsic factors influencing training outcomes, provided an appropriate commencement point for applying the HSI-informed FEA for examining the simulation tools and techniques involved in training. For the present work, the following simulation “categories” were examined:
 Part-task Training Simulators: These trainers simulate one part of a medical environment, usually a specific part of the body. Some models are animated, but they typically do not respond appropriately to users’ actions.
 Computer-Based Systems: These systems train decision-making skills using a computer interface. The programs are used for independent, learner-centered training, and typically provide a limited simulation of a specific medical topic and offer automatic feedback based upon students’ inputs.
 Virtual Reality and Haptic Systems: These systems are more complex computer-based simulations that include specialized input devices (e.g., laparoscopic handles) instead of a standard keyboard and mouse. Typically, the special device provides resistance, giving the “haptic” feel of pushing against bone, skin, or tissue while the virtual procedure is shown on the computer monitor.
 Standardized Patients: Standardized patients consist of human actors playing the roles of ailing patients. Standardized patient actors can be combined with part-task trainers (e.g., a simulated arm that can be sutured), so that the trainees can practice communication and performance skills in combination.
 Integrated Simulators: Integrated simulators are the most complex simulator systems. They combine computerized control and feedback with responsive physical models (typically mannequins) that can be connected to monitoring devices. The most advanced versions can accurately replicate many bodily
2014 Paper No. MS 1401 Page 4 of 9
MODSIM World 2014

functions such as breathing and bleeding.
 Simulated Environments: Simulated environments combine several of the previous techniques into realistic
settings and typically consist of pre-scripted training scenarios, actors, props, full-sized integrated simulators, and monitoring devices (e.g., video and audio recording). These scenarios allow students to integrate all of their skills and practice working under non-ideal conditions.
In 2012, a FEA was conducted to evaluate Army Nursing KSAs, simulation use, and training gaps, particularly in SBT. The findings of the analysis informed recommendations for supporting decisions in the US. Army Nurse Corps (DSCI, MESH Solutions Division, 2013). Though the results of the study are proprietary to the customer and we anticipate forthcoming reports, the methodological approach produced a caliber of qualitative and quantitative results which merit replication of the methods in other contexts. Therefore, it is the way the study was conducted that serves as the objective in the following section.
FEA OF ARMY NURSING TRAINING
Data was collected from Army and civilian nurses, students, faculty, and administrators with the goal of informing Army critical-care nursing. The research team conducted structured interviews, focus groups, survey collection, and site observations to capture a picture of the “state of” simulation-based nursing training. The various collection methods sought information regarding status quo of nursing training, experiences with specific SBT technologies, thoughts on best-practices, and implementation of training protocols. Measures included prior knowledge and experience, expert insight on best practices (including progression of instruction, core KSA at various stages, behavioral indicators of expertise, and training best practices, gaps, inefficiencies and opportunities), reactions to medical simulations (including past experience, satisfaction, ease of use, best practice, value, and perceived effectiveness and efficiency), and observed behavior during training (including learner involvement, content depth, KSAs, and the effectiveness and efficiency of instruction and instructional technologies).
Researchers solicited military participant volunteers from the U.S. Army Medical Department Center and School (AMEDDC&S), located on the Brooke Army Medical Center (BAMC) campus in San Antonio, TX; the Defense Medical Readiness Training Institute (DMRTI), located at Fort Sam Houston in San Antonio, TX; and the Graduate School of Nursing, located at the Uniformed Services University of Health Sciences (USUHS) in Bethesda, MD. Civilian participant volunteers came from the School of Nursing at Johns Hopkins University (JHU) in Baltimore, MD; the College of Nursing at the University of Central Florida (UCF) in Orlando, FL; the Critical Care and Emergency Departments at Louisiana State University Health Sciences Center (LSUHSC) in Shreveport, LA; and the Critical Care and Emergency Departments at Parrish Medical Center (PMC) in Titusville, FL. Participants represented a range of experience using simulation and represented a cross-section of practice.
A practical aspect of the present approach is the system-sensitive manner of data collection. As the study included a range of facility types, the approach needed to have the flexibility to inject into the system with as little obtrusion as possible. Consequently, the final sample (n = 105) participated in different components of research collection, according to availability and opportunity. For example, AMEDDC&S provided the highest number of one-on-one expert interviews, but could not accommodate questionnaire distribution. On the other hand, USUHS had fewer one- on-one interviews but provided fifty questionnaire participants. It is because the FEA is informed by HSI, that the instruments themselves retain validity to be used at different levels according to access and opportunity. Multiple tools of measurement allow for multiple lenses to shine on the same set of concerns and the disparate components become reintegrated during the data analysis phase of the FEA. The use of multiple data sources allows for depth of information type to address the questions required by the FEA (see
Table 1, above).
The findings from the study reflected a blend between SBT challenges, generally, and domain-specific concerns. One of the general findings applies to the development of scenarios. As simulations depend on some degree of real- world representation, the relevancy of scenarios is critical. The BAMC Simulation Center reported 300 saved scenarios. BAMC Simulation Center participants indicated that it takes a collaboration of faculty one week to develop a new scenario and input it into the mannequin system. This burden on resources results in inadequate use of the mannequin’s complex full-simulation capacity as the trainers substitute easier-to-develop part-task simulations. As a result of the FEA, researchers identified a link between scant resources (namely time and SBT instructional design experience) and limited access to nursing scenarios. Possible solutions include the development
2014 Paper No. MS 1401 Page 5 of 9
MODSIM World 2014

of extensive and shared repositories of nursing scenarios and the creation and maintenance of nursing simulation coordinators dedicated to the management of simulation training technologies in the training context.
From FEA to Best Practices in Nursing
Researchers considered the multi-source responses and interpreted the data in a manner that shaped the following recommendations in Army Nursing. The assertions do not assume that these practices are not present in some, or even many training facilities. Rather, the aim is to concisely identify which practices are empirically supported, so that existing simulation-based training efforts may be compared to the data-driven recommendations in order to support decision-making for ongoing and new training.
To that end, participants across the data collection sites identified numerous best practices. The experts identified the following areas where Army Nursing could articulate best practices: progression of learning, universal core KSAs, behavioral indicators of experience levels, assessment, and use of medical simulators. Then, researchers leveraged the the areas of practice and applied the empirical data in a manner that supported the establishment of initial best-practice practice recommendations (
Table 2).
Table 2. Best-Practice Recommendations from Army Nurses Study.
MODSIM World 2014
    Recommendation
      Description
    Write simulation-based training into nursing curricula.
Best use of resources would support linking scenario scripts, technology set up and maintenance procedures, and other simulation-based training media to POIs distributed through TRADOC.
      Develop structured Structured discussions with the trainees and instructors would focus on identifying and debriefing strategies. describing important thoughts that lead to making specific decisions (positive or
negative).
      Use faculty as scenario Use faculty as visiting family members, charge nurses, physicians, or the mannequin role-players. voice is a cost-effective method that would increase the adaptability of the scenarios. Additionally, faculty who are immersed with the students can provide more immediate
feedback and direction during the simulation, and can add to the realism of the case.
      Implement assessment tools targeting critical thinking and decision making skills.
Instructors would complete behavioral assessments of each student, aligned with the specific scenario, as observed during simulation training. This assessment would guide post-scenario debriefing.
      Training facilities would fund dedicated simulation staff at schoolhouses.
All of the interviewed stakeholders reported that they needed more dedicated staff to help manage and deliver the simulation-based training. Established simulation coordination staff would improve simulator performance and support the cognitive/behavioral outcomes of the training, while using personnel resources in the most productive manner.
 The above recommendations fit with SBT gaps identified in previous research (cf. Bremner, Aduddell, Bennett, & VanGeest, 2006; Cant & Cooper, 2010; Jeffries, 2005), but contribute in an important way. The FEA acted as both a process (a means for understanding the problem), a product (a picture of the state-of-training), and a pathway (a plan forward) in a manner that facilitated a timely set of best practice recommendations that could immediately be used by training decision-makers. So, the question arises, would a similarly HSI-informed FEA support other medical fields as effectively? Future work should examine universal features of simulation-based medical training and domain-specific best practices.
OF NURSES AND MEDICS
To build on the work from the Army Nursing research, the next phase of study will aim to explore those universal and domain-specific aspects of simulation-based medical training. The focus of the next phase of work will be Army Combat Medics, a domain chosen because it provides commonalities with and distinctions from Army Nursing.
2014 Paper No. MS 1401 Page 6 of 9

Like Army Nursing, medic programs require training in both medical KSAs and military functions. However, these two domains have unique characteristics and professional requirements. Generally, the medic population also referred to as Health Care Specialist (68W), will be deployed in theater with fewer hours of training than their nursing counterparts. Medic duties focus on the administration of emergency medical treatment to battlefield casualties in a time-constrained context. This is in contrast to the Army Nursing conditions, which tend to be more diverse and within a continuum of care, with specialties in areas such as psychology, emergency room, ob/gyn, and public health. Army Medics undergo ten weeks of Basic Combat Training and 16 weeks of Advanced Individual Training, including patient care practice. Army Nursing positions require completion of a bachelor’s degree in nursing from an accredited school and a current nursing license. In most situations, combat medics will receive specialized training for austere-condition tactical combat casualty care, whereas nurses may practice under austere and/or resource-satisfactory conditions (Smith, 2008).
The training backgrounds for these medical professionals are not the only sources of variance between the two populations. Research indicates that nurses and medics are different across multiple psychometric and deployment measures (Wilmoth, Wilmoth, De Scisciolo, Gilchrest, & Dmochowski, 2007). As these groups differ across multiple features, it is important that researchers avoid conflating them into “medical professionals” and acknowledge both similarities and differences so that decision-makers can be supported when addressing each field. The human-factor facet of the FEA approach accommodates population differences, as its framework builds around the expertise of the human in the system, regardless of the differences among domains. Like the previous work in understanding Army Nursing training, future research on Combat Medics will use the HSI-informed FEA approach for data collection so that the expert reports and objective observations can be applied using a consistent methodology. This will help researchers identify the areas of shared gaps and opportunities and it will help researchers isolate the gaps and opportunities unique to each field. These findings may then be leveraged to the development of decision-making models with the requisite adaptive features to support the specific training domains.
ACKNOWLEDGEMENTS
This work was supported by the U.S. Army Research Development and Engineering Command (RDECOM), Army Research Laboratory (ARL), Human Research and Engineering Directorate ( HRED), SFC Paul Ray Smith Simulation and Training Technology Center (STTC) under contractW911QX-13-C-1001. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of RDECOM, the Army, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon. Special thanks to Dr. Sae Schatz, Chief Scientist at DSCI-MESH for team guidance.
REFERENCES
Achiche, S., Appio, F., McAloone, T., & Di Minin, A. (2013). Fuzzy decision support for tools selection in the core front end activities of new product development. Research in Engineering Design, 24(1), 1–18. doi:10.1007/s00163-012-0130-4.
Barach, P., Satish, U., & Streufert, S. (2001). Healthcare assessment and performance: Using simulation. Simulation & Gaming, 32(2), 147–155. doi:10.1177/104687810103200203.
Booher, H. R. (2003). Handbook of Human Systems Integration. John Wiley & Sons.
Bremner, M. N., Aduddell, K., Bennett, D. N., & VanGeest, J. B. (2006). The use of human patient simulators: Best
practices with novice nursing students. Nurse Educator, 31(4), 170–174.
Cant, R. P., & Cooper, S. J. (2010). Simulation-based learning in nurse education: Systematic review. Journal of
Advanced Nursing, 66(1), 3–15. doi:10.1111/j.1365-2648.2009.05240.
Corlett, J., Palfreyman, J. W., Staines, H. J., & Marr, H. (2003). Factors influencing theoretical knowledge and practical skill acquisition in student nurses: An empirical experiment. Nurse Education Today, 23(3), 183– 190. doi:10.1016/S0260-6917(02)00232-0.
2014 Paper No. MS 1401 Page 7 of 9
MODSIM World 2014

DSCI, MESH Solutions Division. (2013). Front-end analysis for army nursing simulation, Volume 1 (Technical Report No. MESH-DCN_FEA for Army Nursing Sim_ COR_13_0314_01) (p. 95). Orlando, FL.
Fass, D. (2006). Rationale for a model of human systems integration: The need of a theoretical framework. Journal of Integrative Neuroscience, 5(3), 355–372.
FY09 Department of Defense Human Systems Integration Management Plan, Version 1.0. (2009). Washington, DC: ODUSD(A&T), ODUSD(S&T) Director of Biological Systems. Retrieved from http://www.acq.osd.mil/se/docs/FY09-DoD-HSI-Management-Plan.pdf.
Gaba, D. M. (2004). The future vision of simulation in health care. Quality and Safety in Health Care, 13(1), i2-i10 Gordon, J. A., Oriol, N. E., & Cooper, J. B. (2004). Bringing good teaching cases “to life”: A simulator-based
medical education service. Academic Medicine, 79(1), 23–27. doi:10.1097/00001888-200401000-00007
Harless, J. (1978). Motivation and front-end analysis. NSPI Journal, 17(6), 5–6. doi:10.1002/pfi.4180170606
Jeffries, P. R. (2005). A framework for designing, implementing, and evaluating simulations used as teaching strategies in nursing. Nursing Education Perspectives, 26(2), 96–103.
Jimenez, C., Navia-Osorio, P. M., & Diaz, C. V. (2010). Stress and health in novice and experienced nursing students. Journal of Advanced Nursing, 66(2), 442–455. doi:10.1111/j.1365-2648.2009.05183.x
Johannsson, H., Ayida, G., & Sadler, C. (2005). Faking it? Simulation in the training of obstetricians and gynaecologists. Current Opinion in Obstetrics & Gynecology, 17(6), 557–561.
Khamisa, N., Peltzer, K., & Oldenburg, B. (2013). Burnout in relation to specific contributing factors and health outcomes among nurses: A systematic review. International Journal of Environmental Research and Public Health, 10(6), 2214–2240. doi:10.3390/ijerph10062214.
Mager, R. F., & Pipe, P. (1997). Analyzing performance problems, or, you really oughta wanna: How to figure out why people aren’t doing what they should be, and what to do about it. Atlanta, GA: Center for Effective Performance.
Muramoto, D. M. L., Campbell, J., & Salazar, Z. (2003). Provider Training and Education in Disease Management. Disease Management & Health Outcomes, 11(10), 633–645. doi:10.2165/00115677-200311100-00003
Rolfe, G. (1996). Going to extremes: action research, grounded practice and the theory-practice gap in nursing. Journal of Advanced Nursing, 24(6), 1315–1320. doi:10.1111/j.1365-2648.1996.tb01040.
Rouse, W. B. (2007). People and organizations: Explorations of human-centered design. John Wiley & Sons.
Rush, K. L., Adamack, M., Gordon, J., Lilly, M., & Janke, R. (2013). Best practices of formal new graduate nurse transition programs: An integrative review. International Journal of Nursing Studies, 50(3), 345–356. doi:10.1016/j.ijnurstu.2012.06.009.
Salas, E., & Burke, C. (2002). Simulation for training is effective when ... Quality & Safety in Health Care, 11(2), 119–120. doi:10.1136/qhc.11.2.119.
Schatz, S., Marraffino, A., Allen, C., & Tanaka, A. (2013). Human–systems integration, simulation, and the nursing shortage. Proceedings of the International Symposium of Human Factors and Ergonomics in Healthcare, 2(1), 135–142. doi:10.1177/2327857913021026.
Schmorrow, D. D., & Nicholson, D. M. (2012). Advances in Design for Cross-Cultural Activities. CRC Press.
2014 Paper No. MS 1401 Page 8 of 9
MODSIM World 2014

MODSIM World 2014 Sharif, F., & Masoumi, S. (2005). A qualitative study of nursing student experiences of clinical practice. BMC
Nursing, 4(1), 6. doi:10.1186/1472-6955-4-6.
Simmons, A., & Yoder, L. (2013). Military Resilience: A Concept Analysis. Nursing Forum, 48(1), 17–25.
doi:10.1111/nuf.12007.
Smith, K. K. (2008). Critical care nursing in an austere environment: Critical Care Medicine, 36(Suppl), S297– S303. doi:10.1097/CCM.0b013e31817daa01.
Wilford, A., & Doyle, T. J. (2006). Integrating simulation training into the nursing curriculum. British Journal of Nursing (Mark Allen Publishing), 15(17), 926–930.
Wilmoth, M. C., Wilmoth, M. C., De Scisciolo, S., Gilchrest, L. J., & Dmochowski, J. (2007). The Readiness Estimate and Deployability Index and Psychometric Properties in Army Reserve Nurses and Medics. Military Medicine, 172(8), 800–805.
2014 Paper No. MS 1401 Page 9 of 9
