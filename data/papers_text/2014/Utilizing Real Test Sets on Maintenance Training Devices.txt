Utilizing Real Test Sets on Maintenance Training Devices
Thomas Andrew Ferrell Pinnacle Solutions, Inc. Huntsville, Alabama AFerrell@PinnacleSolutionsInc.com
ABSTRACT
The purpose of this paper is to describe how real aircraft test sets may be a viable option for use on simulated maintenance training devices. The decision to utilize two real aircraft test sets on the UH-60M Black Hawk Electrical Trainer (BHET-M) will be discussed in detail. The first section will provide an introduction of the BHET- M trainer and the requirements of the training device. The next section will describe how and why the decision was made to utilize the real aircraft test set versus a simulated test set. This process includes determining the requirements of the test set to support the Maintenance Operational Checks (MOCS) and Fault Isolation Procedures (FIPS) that are to be trained on the device. For example:
(1) How much of the test set functionality is required to meet the MOC/FIP requirement?
(2) Does the overall system design support the test set solution?
(3) What is the test set user interface?
(4) Is trainer unique cabling acceptable?
(5) What is the cost to build a mock-up versus the cost of the real unit?
The next section describes how two real aircraft test sets were designed into the BHET-M simulated aircraft environment. This design can be viewed as reverse stimulation in a sense, where the real test sets are injecting real aircraft signals (electrical signals, air pressure) into the simulated aircraft environment. The following section explains how the test sets were implemented into the system providing details on the hardware data acquisition (DAQ) system and how the software interpreted the real test set data. The next section describes the integration of the test sets and problems encountered during the integration phase. The last section describes the test phases, how the test sets performed in a full-up environment, and how the customer reacted to the use of the test sets. Finally the conclusion discusses the overall successes, failures and whether the same decision to utilize the aircraft test sets would be made in hindsight.
ABOUT THE AUTHOR
Thomas Andrew Ferrell is the Director of Engineering at Pinnacle Solutions, Inc. located in Huntsville, Alabama. Mr. Ferrell is a senior systems engineer with over 25 years of systems, hardware, and software design, development, integration, verification, validation, and test experience in the aviation and aviation simulation arenas. Mr. Ferrell is currently serving as Technical Program Manager (TPM) on the BHET-M program.
2014 Paper No. MS1449 Page 1 of 9
MODSIM World 2014

Utilizing Real Test Sets on Maintenance Training Devices
Thomas Andrew Ferrell Pinnacle Solutions, Inc. Huntsville, Alabama AFerrell@PinnacleSolutionsInc.com
INTRODUCTION
The UH-60M Blackhawk Electrical Trainer (BHET-M) is a training device developed for the United States Army 128th Aviation Training Brigade to meet the training requirements for Military Operational Specialty (MOS) 15F10 (Aircraft Electrician) and 15N10 (Avionics Mechanics) students. The training task list for BHET-M is system familiarization, component identification, servicing, Maintenance Operational Checks (MOCs), Fault Isolation Procedures (FIPs), Remove/Install (R/I) and repair (AVNS-PRF 10270). A detailed engineering analysis of the MOCs and FIPs required by the BHET-M Performance Specification (P-SPEC) and Fidelity Matrix (FM) identified eleven test sets to support MOCs/FIPs training. The final analysis resulted in the utilization of nine Commercial- Off-The-Shelf (COTS) aircraft test sets and two simulated test sets. Two of the test sets chosen as COTS solutions that will be our focus are the Pitot Static Test Set (Druck ADTS 405) and Ultrax EICAS/CEFS Test Set (UxValidator Series 2). The decision-making process steps followed that steered these two test sets to a COTS solution is discussed in theory and then specifically to the two test sets identified. Table 1 identifies the test sets and the solutions.
MODSIM World 2014
    TEST SET EQUIPMENT
       WORK PACKAGE(S)
      SIMULATED/REAL
    Test Set, Pitot-Static, Druck 405
     107
    REAL
    APU Test Set, Simulator, Temp./Speed, H296B-1
   155
  SIMULATED
    Power System Analyzer, 60B63-5A
     114
    SIMULATED
    Fuel Quantity Test Set, PSD60-1AF
       136
      REAL
    Ultrax EICAS/CEFS H60M Set, 08-0819-02
     109, 113, 164
    REAL
    Ultrax UxValidator Series 2, 10-0210-02 (needed for use with EICAS/CEFS Test Set)
   109, 113, 164
  REAL
    Locally-Made Infrared Light Test Set, (WP 1606 00, Figure 101)
     123
    REAL
    Infrared Light, Test Set, (WP 1602 00, Figure 96)
       124
      REAL
    IR Orientation Lights Test Set, Sikorsky T7055- 01087
   124
  REAL
    Phase Rotation Meter (PSI) Knopp Model K6
       164
      REAL
    Test Set, Sonar, TS200
     113
    REAL
  Table 1-BHET-M Test Sets
DECISION-MAKING PROCESS
The primary decision-making factors used to determine the best COT/Simulated test set solution requires a five step process. The first step is to identify how much of the overall test set capability is utilized by the MOCs/FIPs requirements of the training device. The second step is to determine if the overall training device systems and subsystems will support the test set solution. The third step is to evaluate the complexity of the user interface. The fourth step is to determine if trainer unique cabling solutions will be required. The last step is to perform a cost- benefits analysis between utilizing the COTS test set versus designing a simulated test set.
Test Set Capability Requirements
2014 Paper No. MS1449 Page 2 of 9

In this step of the process the goal is to determine the requirements of the test set in the training device environment. This is normally called out in a Performance Specification (P-SPEC) provided by the customer. The P-SPEC often contains the Training Task List (TTL) which describes each Work Package the student must perform. The best method to determine the required test sets and overall capability required by the test set for the specific training device is to work step-by-step through the MOCs/FIPs that utilize the test set. If the majority of the test set capability is utilized and complex, the test is probably a good candidate for the COTS solution. However, if the test set is very simple, the majority of the capability may be utilized while still being a simulated test set candidate.
System/Subsystem Test Set Support
The purpose of this step is to identify the system and subsystem capabilities of the training device and how the capabilities support or does not support the test set solution. Various parameters impact the ability of the training device to support the required test sets and thus the real versus simulated decision. The parameters include the electrical, mechanical, pneumatic and hydraulic architecture of the training device. The architecture of the training device is the key decision point in determining which type of test set is the best solution. The electrical architecture is analyzed to determine the type of power available (400 Hz 3Φ AC, 60 Hz AC, or DC). The electrical architecture determines which type of test set would work best for power measurement or power analyzer test sets. The mechanical system architecture is evaluated to determine capabilities of systems such as flight controls, flight control rigging and flight control feedback. These capabilities determine which type of test set is best suited to measure travel and forces. The pneumatic architecture is analyzed to determine if systems that utilize air or gas pressure are available. The pneumatic architecture determines if test sets that measure air or gas under pressure can be real or must be simulated. This includes systems such as the pitot static system and would determine the type of pneumatic test set that are required. The hydraulic system architecture is analyzed to determine if the training system is plumbed for hydraulic pressure. This includes functional systems such as braking, flight controls, auxiliary power unit, and transmission. Training devices are often referred to as “wet” systems if working hydraulic systems are available or “dry” systems if hydraulics is not available. The availability of hydraulics on the training device directly influences the selection of the test set to support the hydraulic systems. The output of the analysis of defines the training device and test set interface and greatly impacts the final real versus simulated test set decision.
User Interface Complexity
In this step of the process, the complexity of the test set user interface is analyzed. The user interface complexity refers to the level of detail of the user interface built into the test set. Test set user interfaces range from simple analog gauges, often referred to as “steam” gauges, and discrete switches to high fidelity Graphical User Interface (GUI) displays. This decision point is directly based on the fidelity of the interface. Test sets with analog/discrete interfaces are much easier to simulate then test sets with complex GUI displays. Reverse engineering a complex multiple page GUI tends to steer the decision to a real test set.
Trainer Unique Cabling
This step is used to determine if trainer unique cabling will be required to interface the training device with the test set. The requirements for trainer unique cabling are often included in the P-SPEC for the training device. Trainer unique cabling may or may not be allowed on the training device. In the BHET-M, no trainer unique cabling is allowed for the training device to test set connections. All of the test sets had to be connected to the training device via cables, connectors and plugs physically identical to the aircraft. However the electrical signals in the cable only had to match the aircraft only if the signal had to be measured. In test sets with electrical interfaces, this is typically not a problem if the solution is real or simulated. Extra pins can be utilized for custom communications protocols such as serial (RS-233/422/485) or Ethernet in simulated test sets. Test sets without electrical interfaces and training device requirements for no trainer unique cabling introduce the challenge. In this case for pneumatic or hydraulic test sets the solution may require hidden pumps. Another solution for these conditions is a wireless network but this solution is difficult to utilize because of Information Assurance requirement.
Cost/Benefit Analysis
The final step is to analyze the data gathered in the first four steps and determine the cost/benefits of utilizing real or simulated test sets. Typically test sets that have electrical interfaces and utilize a simple gauge/switch user interface
2014 Paper No. MS1449 Page 3 of 9
MODSIM World 2014

are geared for simulation. They are less expensive to spec and build than purchasing the actual unit. They can have serial or Ethernet interface much easier to integrate with the training device host simulation which requires fewer man-hours for integration. An additional benefit is simulated test set don’t have to be shipped back to the manufacture for calibration. On the other hand, test set that have electrical interface but numerous signals of different types (resistance, frequencies, various voltage levels) and complex user interfaces are likely to be more expensive to simulate than utilize the real test sets. In both of these situations the cost/benefits must be weighed in detail. Often the test sets without electrical interfaces, pneumatic or hydraulic, leave no other design choice than utilizing the real test set. In these cases the simulated aircraft must be fitted or plumbed with equipment to support the real test set.
APPLYING THE PROCESS
In the following sections an example of how the process was applied during the requirements analysis, design, and integration and test phases for two tests sets utilized on the BHET-M program is described in detail. The two test sets were the Pitot Static Test Set (Druck ADTS 405) and EICAS/CEFS Test Set (UxValidator Series 2).
Pitot Static Test Set
The Druck is a two-channel (Ps, Pt) pressure control system used to verify the Pitot Static System (Druck User Manual). The Druck tests the Pitot Static System plumbing for leaks as well as provides the Pt and Ps pressure to drive the pressure sensitive instruments. The instrument values, Rate of Climb (ROC), altitude and airspeed air verified on the UH-60M Electronic Standby Instrument System (ESIS) and the UH-60M baseline Multi-Function Displays (MFDs) (K114). The Druck ADTS 405 is illustrated in Figure 1.
Figure 1 - Druck ADTS 405.
Requirements Analysis
During the requirements analysis phase of the BHET-M program, the five step decision making process was utilized to determine the real versus simulated solution for the Druck test set. The first step was to determine the test set capability requirements. Utilizing the Technical Manual, Aviation Unit and Intermediate Maintenance Manual For Helicopters (TM 1-1520-280-23-1) and the guidance of the P-SPEC and Fidelity Matrix, the appropriate Work Package (WP 0107 00)( TM 1-1520-280-23-1), Pitot Static System was analyzed to determine the test set capability. The WP utilized complete functionality of the relatively complex test set including the use of the handheld unit.
MODSIM World 2014
 2014 Paper No. MS1449 Page 4 of 9

Therefore, step one provided an early indication that the Druck was potentially a candidate for the real test set solution. The second step was to determine if the training device system/subsystems supported a real or simulated test set solution. The government provided a UH-60L airframe that was retrofitted into a simulated UH-60M airframe. The original UH-60L airframe already had the components of the Pitot Static system (heads, lines, chambers, restrictors) in place and in good condition. Like step one, step two provided an additional indicator that the Druck was again a good real test set solution. The third step was to analyze the complexity of the Druck user interface. The Druck contained a fairly simple interface that could have easily been reproduced with some level of software effort. However, the Druck also has a wired remote handheld unit that would have added additional software effort. Although not as clear as the results from steps one and two, step three still pointed towards the real test set solution. Step four was to determine the trainer unique cabling that required by the Druck test set. The Druck test set has no electrical connection to the aircraft. The Druck connects to the aircraft through hoses provided in the Druck adaptor kit to the Pitot and Static fittings on the pilot and copilot pitot heads. A simulated Druck test set design required electrical signals hidden in the hoses and hidden connectors and plugs at the heads and simulated test set. The information from the analysis of step four was the most resounding indicator that the real Druck test set was the best solution. Due to the overwhelming conclusion based on the first four steps of the process, the cost/benefit analysis was not performed. The BHET-M would utilize the real Druck test set. The electrical, mechanical and software requirements were defined based on the real test set solution.
Design
During the design phase the electrical, mechanical and software requirements based on utilizing the real Druck test set were built into a system preliminary and detailed design. The design was to refurbish the pitot static pneumatic system in the airframe. The system would be plumbed through Precision Pressure Transducers (PPT) to read the Pt and Ps air pressure. The Host Simulation model (Electrical, Air Data Computer, and Real-Time IO) would interact with the pneumatic and electrical design. The PPTs would be powered through the Data Acquisition System via the simulated electrical system model. The Air Data Computer (ADC) models would read the PPT voltage through the Cockpit IO model from the DAQ. The ADC would then scale the pressure transducer voltage into Pt, Ps and calculate differential pressure to drive the rate of climb, altitude, and airspeed indicators on the simulated ESIS and MFDs. In summary, the real Druck would provide the real air pressure through the real aircraft pitot static system and the PPTs. The software would monitor the PPT voltages, scale the voltage to pressure, calculate the differentials for each instrument, and drive the simulated indicators.
Hardware Software Integration
The hardware software integration (HSI) phase began in the Software Integration Lab (SIL) with the computational systems (Host, MFD, and ESIS), PPTs, Druck, and a mocked up pneumatic system with short hoses. The SIL testing during early HSI proved the real test set solution was viable and relatively simple. Once integration moved to the hangar on the training device there were some initial problems with leaks in the system. The first test attempt with the Druck was the leak test and the device failed the test. A UH-60M Maintenance Technician was brought in to check out the plumbing. A few connectors were not sealed properly. Once corrected, the Druck leak test was attempted and the leak test passed. Software was tweaked to scale the pressure from the PPTs in the airframe and the rate of climb, altitude and airspeed indications were verified on the simulated ESIS and MFDs to match the Druck commanded rate of climb, altitude and airspeed. Utilizing the real test set, the real aircraft pitot static pneumatic system, and real PPTs to provide Pt and Ps pressure to a simulated Air Data Computer provide an indication of early success and the correct decision.
Test
The test phase for the BHET-M includes Contractor Preliminary Inspection (CPI) and Government Preliminary Inspection (GPI). BHET-M is currently in the CPI phase. Both of these phases utilize an Acceptance Test Procedure (ATPr) to test each requirement of the training device. The BHET-M ATPr is written to utilize the WPs in the Maintenance manuals with as little training unique steps as possible. The WP that include use of the Druck test set, WP 0107 00 has been run end-to-end. The Druck is connected to the pitot heads with the adapter kit per the WP aircraft instructions. The leak test results are within tolerance. Rate of climb, altitude and airspeed indications match all rates and tolerances of the WP. The test has been run by operations with previous UH-60M maintenance technician experience.
2014 Paper No. MS1449 Page 5 of 9
MODSIM World 2014

EICAS/CEFS Test Set
The EICAS/CEFS Test Set is composed of the Ultrax UxValidator and the EICAS/CEFS Pod set. The UxValidator contains the processor and GUI display. The Pod Set contains a BrainPack, cabling and interface Pod that plugs into the UxValidator. The EICAS/CEFS test is used to troubleshoot and verify the functionality of the UH-60M Data Concentrator Unit (DCU) and Crashworthy External Fuel System (CEFS). The Ultrax test set transmits and receives signals to the DCU per the UH-60M DCU Detailed Specification to drive the Engine Indication and Crew Alert System (EICAS) signals. The Ultrax also transmits and receives signals through the CEFS Relay panel to support testing of the CEFS. The majority of EICAS signals are transmitted from the Ultrax and verified on the UH-60M Baseline MFDs. The majority of CEFS signals are received from CEFS system through the CEFS Relay panel and verified on the UxValidator display (PN-08-0819-02). The Ultrax UxValidator is illustrated in Figure 2.
Figure 2 - Ultrax UxValidator
Requirements Analysis
During the requirements analysis phase BHET-M program, the five step decision making process was utilized to determine the real versus simulated solution for the Ultrax test set. The first step was to determine the test set capability requirements. Utilizing the Technical Manual, Aviation Unit and Intermediate Maintenance Manual For Helicopters (TM 1-1520-280-23-1) and the guidance of the P-SPEC and Fidelity Matrix, the appropriate Work Packages (WP 0109 00, WP 0113 00 and WP 0164 00), Engine Instrument and Crew Alert System, Instrument System Integrated Vehicle Health Management System, and CEFS Fuel Quantity and Fuel Management were analyzed to determine the test set capability. The three combined WPS utilized complete functionality of the relatively complex test set including the use of all electrical signals. Therefore, step one provided an early
indication that the Ultrax was potentially a candidate for the real test set solution. The second step was to determine if the training device system/subsystems supported a real or simulated test set solution. The BHET-M architecture did not include the aircraft DCU thus it did not support the test set directly. However, the aircraft did contain a modular DAQ system in the architecture which would support the test once the appropriate IO cards were identified and installed. Like step one, step two provided an additional indicator that the Ultrax was again a good real test set solution. The third step was to analyze the complexity of the Ultrax user interface. The Ultrax contained a complex, multi-page interface for both BrainPacks that would require a significant software effort. Step three was another clear indictor for the real test set solution. Step four was to determine the trainer unique cabling that required by the
MODSIM World 2014
 2014 Paper No. MS1449 Page 6 of 9

Ultrax test set. The Ultrax test set contained spider harnesses for connection to both the DCU and CEFS Relay panels. Both test set harnesses had sufficient pins to support a simulated solution. In fact, modifying the actual test set BrainPack and PODS to support a training device by adding a serial or Ethernet interface would have been the ultimate solution. However, at the time, the vendor did not have the time or manpower to support this modification. The outcome of step four was that the cabling supports both the real or simulated test set. Due to the overwhelming conclusion based on the first three steps of the process and no interest by the vendor to support the simulation modification, the cost/benefit analysis was not performed. The BHET-M would utilize the real Ultrax test set. The electrical, mechanical and software requirements were defined based on the real test set solution.
Design
During the design phase the electrical, mechanical and software requirements based on utilizing the real Ultrax test set were built into a system preliminary and detailed design. The design was to utilize the real Ultrax test set by capturing the real test set electrical signals at the DAQ. A dedicated Ultrax DAQ cube was developed to pass each test set signal to/from the Host Simulation through the Real-Time IO model. The DAQ was located onboard the airframe in the bay area that normally holds the number two fuel cell. The test spider harnesses connected to a mock-up DCU and CEFS Relay Panel but were modified to pass-through the information to the DAQ. This provided the student the ability to connect the test set per the MOC. The majority of the signals from the Ultrax test set for the EICAS were analog inputs. The analog input types were AC voltage, DC voltage, DC thermocouple, AC Frequency and Resistance. DAQ IO cards were selected that would meet the requirements of the numerous analog input signal types. In addition a voltage to frequency converter module was added to keep that processing out of the DAQ and Host Simulation. The DAQ solution also included an add-on card to handle resistance and thermocouple measurement. The Host Simulation reads the EICAS Ultrax test set signals from the DAQ, scales them per the DCU Detailed Specification, calculated to appropriate units by the engine/transmission models and passes them to the simulated MFD EICAS page for display. The other inputs from the test set for EICAS were discrete inputs. These are read from discrete input cards located in the dedicated Ultrax DAQ and are handled by the host in the same manner as described for analog inputs. Examples of the EICAS analog inputs are fuel quantity (main and external), engine oil pressure, engine torque, engine turbine gas temperature, engine gas speed, engine rpm, engine oil temp, transmission oil temp, transmission rotor speed, and transmission oil pressure. Examples of EICAS discrete inputs include fuel tank identifiers (main and external), fuel vent overflow, rotor speed reset. The Ultrax CEFS model provides discrete inputs for fuel tank empty, start/stop fuel transfer, and reads discrete inputs for fuel valve status. The DAQ, Real-Time IO and Host Simulation design was the same for the Ultrax CEFS POD through the Host Simulation Fuel model. Another important part of the design was the addition of ground loops in each of the DCU and CEFS Relay panel cables used as a CONNECT/DISCONNECT status through the DAQ. This allowed the Host Simulation to detect when to use the Ultrax Test input/outputs versus the simulated engine, transmission or fuel model input/outputs.
Integration
The hardware software integration (HSI) phase began in the Software Integration Lab (SIL) with the computational systems (Host, MFD, and ESIS), DAQ and Ultrax UxValidator, EICAS Pod, CEFS Pod, spider cables and a test-bed unique set of cabling used to connect the spider cables straight to the DAQ. As expected there were no issues with the analog AC inputs, DC inputs, or thermocouple inputs from the Ultrax EICAS test set. They were read from the test set through the DAQ, scaled per the DCU specification and drove the EICAS indicators as expected. There were minor electrical details to work through on the resistance circuit based on the wiring of the RTD add-on card but those were overcome quickly. The biggest challenge was the frequencies. The frequencies for the transmission rotor speed, engine gas speed, and engine rpm were in the kilohertz frequency range. In the initial attempts at reading the frequencies through the DAQ the analog input IO card could not read the frequencies at a rate fast enough to read real-time. The analog input card was replaced with a faster card. The new card could read the frequencies but it could only buffer them to pass to the Host Simulation IO. It could not provide the frequencies real-time. This Host Simulation to process the buffered frequencies prevented it from running real-time. It was immediately determined that we needed to offload the frequency to voltage conversion out of the DAQ IO card and Host Simulation. The solution was an inexpensive frequency to voltage converter module. The Ultrax EICAS frequency signals were wired to the frequency to voltage converter input. The output of the frequency to voltage converter was wired to the input of the DAQ analog input card. The Host Simulation real-time IO model read the voltage and scaled to frequencies. Then the respective engine and transmission models converted the frequencies to
2014 Paper No. MS1449 Page 7 of 9
MODSIM World 2014

the speed values. The speed values were passed to the MFD for the indicators on the display. The Ultrax EICAS and CEFS test set has not been tested on the device as of this writing. The spider harnesses have not been completed for the device test to begin.
Test
The test phase for the BHET-M includes Contractor Preliminary Inspection (CPI) and Government Preliminary Inspection (GPI). BHET-M is currently in the CPI phase. Both of these phases utilize an Acceptance Test Procedure (ATPr) to test each requirement of the training device. The BHET-M ATPr is written to utilize the WPs in the Maintenance manuals with as little training unique steps as possible. The WPs that include use of the Ultrax test set, WP 0109 00, WP 0113 00 and WP 0164 00 will be run end-to-end. The Ultrax will be connected to the DCU and CEF Relay panel with the spider harnesses per the WP aircraft instructions.
CONCLUSION
The following sections discuss the overall successes, failures and whether the same decision to utilize the aircraft test sets would be made in hindsight.
Successes
The decision to utilize the real Druck and Ultrax EICAS/CEFS test sets on the BHET-M program has been a successful endeavor. The process to make the decision of utilizing the real versus simulated test set has been effective. This process was used to determine the real versus simulated approach for all BHET-M test sets. Only two test sets were simulated, the auxiliary power unit (APU) test set and the Power System Analyzer test set. Utilizing both the Druck and Ultrax EICAS/CEFS test sets has resulted in an overall simple design. The integrity of the test sets remains intact and the Host Simulation software to support them was a much less significant effort. The training utilizing the real test sets cannot provide any negative habit transfer.
Failures
No failures have yet been identified based on the solution to utilize the real Druck and EICAS/CEFS test sets. The only negative impact identified thus far is that the test sets will have to be returned to the Original Equipment Manufacturer (OEM) annually for calibration. Not a failure, but another minor problem is that since the Druck does not have any electrical contact, it was impossible to detect that the Druck test set was connected to the aircraft. Unlike the Ultrax which uses a CONNECT/DISCONNECT loop through a discrete input on the DAQ. Due to this a “Druck Connected” button was added to the GUI on the Instructor/Operator Station (IOS) GUI page. This is how the Host Simulation detects when to use the Druck PPT input versus the ADC model input.
Hindsight
Looking back, I truly believe that the same real versus simulated test set decisions and resulting solutions would be made for all BHET-M test sets and especially for the Druck and Ultrax CEF/EICAS. Although the Ultrax EICAS/CEFS solution was the best decision when the decision was made, I would really have preferred utilizing the real test set with the communications protocol modification. Basically the real test set utilizing spare pins in each POD for serial or Ethernet communications with packet containing the Ultrax signals in aircraft units (rpm, temperature, pressure, etc.). This would have greatly reduced the hardware cost and simplified the host software.
2014 Paper No. MS1449 Page 8 of 9
MODSIM World 2014

REFERENCES
Performance Specification Item Specification for the UH-60M Black Hawk Electrical Trainer (BHET-M), AVNS- PRF 10270, 2 February 2011, Utility Helicopters Project Managers Office
Technical Manual, Aviation Unit and Intermediate Maintenance For Helicopters, Utility Tactical Transport, TM 1-1520-280-23-1, 14 August 2009, Headquarters, Department of the Army
Maintenance Manual for the H-60M EICAS, CEFS System, PN-08-0819-02, Rev 4, Dec 7 2010, Ultrax Aerospace Incorporated
Druck ADTS 405 Air Data Test Systems User Manual K114, Issue No. 9, GE Sensing, General Electric Company
2014 Paper No. MS1449 Page 9 of 9
MODSIM World 2014
