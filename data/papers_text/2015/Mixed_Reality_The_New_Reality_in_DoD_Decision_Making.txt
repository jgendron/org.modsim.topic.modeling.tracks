Tracy A. Lenuik Georgia Tech Research Institute
Quantico, VA tracy.lenuik@gtri.gatech.edu
Luis E. Velazquez Marine Corps Systems Command
Quantico, VA luis.velazquez@usmc.mil
ABSTRACT
Rodger Willis2 Design Mill, Inc.
Dubuque, IA1; Charleston, SC2
sam_murley@designmillinc.com, nathan_greiner@designmillinc.com, rodger_willis@designmillinc.com
Mixed Reality: The New Reality in DoD Decision Making
Samuel R. Murley1, Nathan Greiner1,
Decision making often requires an understanding of where we are, where we want to go, and the options for traversing this distance. Where augmented reality enhances a view of ‘where we are’ and virtual reality creates a view of ‘where we want to go,’ Mixed Reality serves the user who needs to see both. The United States Marine Corps is embarking on this new reality to enhance their trade space analysis tool known as FACT – Framework for Assessing Cost and Technology. With the upgrade to Mixed Reality in FACT, users view new designs (‘where we want to go’) overlaid on existing systems (‘where we are’) to best understand design options and implications. This paper details the development process and reviews potential future applications of Mixed Reality.
ABOUT THE AUTHORS
Tracy A. Lenuik is a Research Scientist at the Georgia Tech Research Institute (GTRI). Working out of GTRI’s Quantico, VA Field Office, her primary area of focus is supporting the U.S. Marine Corps in Systems Engineering. Tracy serves as GTRI’s Visualization Lead for the U.S. Marine Corp’s Framework for Assessing Cost and Technology (FACT). She earned a B.S. in Engineering Physics from the Colorado School of Mines and a Professional Masters in Applied Systems Engineering from Georgia Tech.
Luis E. Velazquez is a federal employee (NH-IV) for the Marine Corps serving as the Deputy Director for Modeling and Simulations (M&S) Division, Marine Corps Systems Command (MCSC), Systems Engineering, Interoperability, Architecture and Technology (SIAT). Over 28 years of professional experience between the Navy, Marine Corps, Industry, and Federal services. Subject Matter Expert (SME) with extensive engineering experience in employing LAN/WAN, software engineering, and systems integration performing advance services to include Live, Virtual, and Constructive (LVC) simulation systems integration and interoperability with tactical C4i devices.
Samuel R. Murley is the Research and Development Manager at Design Mill, Inc. His specialty is developing software that connects the physical and digital worlds through mobile platforms and three-dimensional technology. Sam primarily focuses on identifying and integrating innovative technology into client applications and creating strategic plans to keep ahead of the competition. He earned a B.S. in Computer Science from Clarke University and has over 10 years of experience in his field.
Nathan Greiner is the President of Design Mill, Inc. He started his career within the Automotive Industry, focusing on Project and Process engineering, finishing in Program Management prior to starting Design Mill in 2006. He transitioned into supporting the Department of Defense via interactive visualization and has created software programs for interactive training, interactive maintenance as well as mobile applications for the educational market. More recently he made the shift into the educational and manufacturing industries developing three-dimensional software solutions for Design Mill’s customer base.
Rodger Willis is the Operations Manager of Design Mill, Inc. Charleston. Rodger graduated from Clemson University with a B.S. in Mechanical Engineering. He has extensive experience with engineering design, sales, and management. Many of these engineering projects include creating three-dimensional CAD to support military vehicle design upgrades, and developing interactive software using augmented reality for mobile training. Rodger leverages his training and experience acquired by working at Fortune 500 companies to help Design Mill become one of the top small businesses in its expertise.
2015 Paper No. 19 Page 1 of 9
MODSIM World 2015

Tracy A. Lenuik Georgia Tech Research Institute
Quantico, VA tracy.lenuik@gtri.gatech.edu
INTRODUCTION
Luis E. Velazquez Marine Corps Systems Command
Quantico, VA luis.velazquez@usmc.mil
Rodger Willis2 Design Mill, Inc.
Dubuque, IA1; Charleston, SC2
sam_murley@designmillinc.com, nathan_greiner@designmillinc.com, rodger_willis@designmillinc.com
Mixed Reality: The New Reality in DoD Decision Making
Samuel R. Murley1, Nathan Greiner1,
The U.S. Marine Corp’s Framework for Assessing Cost and Technology (FACT) is a browser-based decision support tool to help guide conceptual and design decisions for the Acquisition Community (term used broadly here to include the elements of acquisition, sustainment, and evolution). U.S. Marine Corps requirements dictate the need for a multi-faceted visualization capability within FACT to further enhance problem-space understanding and realize additional program savings. Foundational to this visualization capability within FACT is the development of the Platform-Independent Visualization (PIV) framework (Lenuik & Murley, 2014). Developed for the purpose of achieving true platform and browser independence, the PIV framework relies on standardization and commonality for all assets (defined here as data, models, and rich media). Lenuik and Murley (2014) describe the full PIV capability in FACT to include the following features:
o Supportive of all types of media and visualization technologies
o Leverages reusable assets throughout all levels of visualization and interactivity
o Visualization viewable from any hardware device or browser
o Reusable by other Department of Defense (DoD) software and systems outside of FACT
o Supporting two-way communication between user and assets
o Ease of integration of new technologies into the visualization layer
o Data and assets managed independent of FACT
o Integration with other systems and data accepted through common web communication protocols (web
services)
The PIV framework was designed specifically with augmented reality and virtual reality in mind. Augmented reality is the layering of digital data onto the real world and is accomplished through facial, image, location- and object- based recognition technology. The digital information can be in the form of video, text, or animated three- dimensional models. Unlike augmented reality where digital content is overlaid onto the real world, virtual reality is completely closed off from the real world environment and is instead a virtual world that can mimic real world scenarios or fictional situations.
Figure 1 illustrates Mixed Reality, which is the merging of real, augmented, and virtual worlds to produce an environment where physical and digital objects coexist. Mixed Reality provides live updates to designs and allows stakeholders to have a fully immersive approach to system design.
Figure 1. Mixed Reality
MODSIM World 2015
 2015 Paper No. 19 Page 2 of 9

Where augmented reality enhances the view of the existing design and virtual reality creates a view of potential new designs, Mixed Reality provides a view of the proposed system (digital design in virtual world) overlaid on to the existing system (fielded system in real world). The potential to use Mixed Reality in DoD applications is largely untouched. For instance, Mixed Reality may serve to close the ever widening gap between designers and users of fielded systems by bringing them onto the same page digitally. By starting with a tradespace analysis tool like FACT, and adding affordable hardware for Mixed Reality, a collaborative environment to host stakeholder interaction can be created.
MIXED REALITY FRAMEWORK DESIGN
Figure 2 illustrates the Mixed Reality framework. This framework is composed of four architectural areas: Configuration Control, External Databases, Hardware Interfaces, and External Software. The key to providing a rich and scalable framework is to develop specific interfaces for each of the four areas. These interfaces act as commonality routers and are responsible for ingesting unique sets of data and converting that data to a single, common set for usability in the visualization engine. This decoupled approach ensures optimal reuse and modularization across different programs.
The user can interact with the content in the visualization platform through various hardware devices as the framework has been developed to be platform-independent. In addition, the PIV framework allows multiple users to join a Mixed Reality scene through different hardware devices, e.g., laptops, augmented reality/virtual reality/Mixed Reality glasses, mobile devices, etc.
Figure 2. Mixed Reality Framework
Configuration Control
FACT Assets Configuration Management
Configuration control is vital to properly updating and sourcing thousands of three-dimensional, two-dimensional, and mixed media assets within FACT servers. The Windchill Product Lifecycle Management (PLM) software manages all assets for change management and control of engineering change proposals. There are many updates and variations that occur with tradespace analysis conducted across thousands of systems, which requires close management to ensure the correct design is being sourced and displayed. Configuration processes allow for seamless control, tracking and approving of each system design. The Windchill PLM is used widely across the services and
MODSIM World 2015
 2015 Paper No. 19 Page 3 of 9

enables integration of existing three-dimensional vehicle and equipment models in FACT. Using one management system promotes commonality between system designs and prevents duplication of modeling efforts.
Software Version Control
The software and light-weight three-dimensional models are version controlled to support a continuous delivery lifecycle. The configuration management platform used to host version control is Seapine’s Surround SCM. With Surround SCM, as with most version control systems, distributed developers version control their code and submit stable updates.
For security and management of code changes, a branching structure is used. Each developer has a private workspace branch where checking in code changes can be done without affecting other developers. Private workspace branches also support the promotion and rebasing of code between branches. Thus, a private workspace branch can receive an update from the mainline branch through a rebase and a private workspace branch can push an update to the mainline branch through a promotion.
The mainline branch hosts all stable code and can be considered the integration branch. All code from this branch receives integration testing. Once testing is cleared, a workspace branch is created as an editable snapshot of the stable and deployable code at that time. This ensures continued management of multiple code bases or software versions. At deployment all code is pulled and compiled from the workspace branch and a snapshot branch is created. This is a read-only branch and acts as an archive of the build at time of deployment.
Data Archiving
Each data point in a tradespace analysis is critical to the lifecycle of a project and product. FACT is a framework for integrating agnostic models in a tradespace analysis tool. Some examples of agnostic models are human models, blast simulation models, or antenna test characterization models. Data from such models comes from high performance computing centers and could take months or years to produce surrogate models (e.g., look-up tables) that can be easily integrated into FACT. The objective with utilizing surrogate models is to define a 70-90% design solution without having to perform finite element analysis or use a high performance computing center. Once the design is agreed upon, a FACT user can then reach out to a simulation center for the high fidelity model. Engaging the high performance computing center at the right time allows the FACT user to get a better design in less time and for less cost.
External Databases
The data input layer has and should continue to be approached from an object-oriented, or scalable, perspective. In other words, no one database is required to supply data into the visualization framework; rather, the visualization framework needs to be compatible with numerous database interfaces. Thus, a common interface is to be written and updated as new data sources are integrated. This common interface should be written as an application program interface (API) and initially support relational and non-relational data interfaces. Currently the framework supports MySQL, MongoDB, and SQL server directly, as well as JSON files. With a common API in place, the visualization interface could support other databases with simple commonality updates to the API interface.
Hardware Interfaces
Historically, hardware interfaces used for system design applications have been virtual reality caves that are both very costly and very large. With the onset of wearable technology over the last three years, the cost associated with similar experiences has plummeted due to new hardware platforms emerging. The current Mixed Reality development of the model agnostic visualization layer led to the determination that exporting to multiple hardware solutions allows for the most impact and versatility. Some of the hardware types include personal computers, wearables, simulators, RGD-D, sensors, augmented reality glasses, and mobile devices. Table 1 provides a sampling of the augmented reality, virtual reality, and Mixed Reality hardware platforms available on the market today and brief snapshot of their key features.
2015 Paper No. 19 Page 4 of 9
MODSIM World 2015

External Software
Table 1. Sampling of Currently Available Hardware Platforms
Software interfaces allow the FACT user to leverage software that exists outside FACT. By creating a software interface, FACT may utilize commercial-off-the-shelf (COTS) software to integrate system architectures. For full scalability and growth, a generic software interface layer is currently being written and will be released with a REST API webservice to connect between FACT and other existing systems. This webservice will enable ubiquitous cross platform communications from FACT to almost any other platform to include high performance computing (HPC). This approach ensures that the software interfaces can coexist in their own environments while leveraging existing features between them to uniquely address a use case. In addition, FACT is implementing a configuration management (CM) strategy to manage all software code. CodeBeamer, a Subversion methodology for configuration management, is currently being explored and implemented to enable multiple developer collaboration on the enhancements to FACT.
IMPACT OF MIXED REALITY
Typically, system designers do not receive user feedback until the prototyping stage. Figure 3 details the impact of utilizing Mixed Reality as part of the design review process.
2015 Paper No. 19 Page 5 of 9
MODSIM World 2015
Hardware Type Key Feature(s)       Source Device
              DAQRI Smart Helmet
Epson Moverio BT-200’s Oakley Airwave
Oculus Rift
Razer OSVR
Sony Project Morpheus
Vuzix M100
Augmented Reality
Durable for industrial use cases (“DAQRI Smart Helmet,” 2015)
Extended battery life       (“Epson Moverio BT-200 Next Generation Smart Glasses,” 2015)
Durable for outdoor use cases       (“Oakley Airwave 1.5,” 2015) 1080p resolution and low       (“The All New Oculus Rift Development
persistence of vision Kit 2 (DK2) Virtual Reality Headset,” 2015)
Fully open source hardware and (“Razer OSVR,” 2015) software platform
                               Large crisp HD display and field of view (90 degrees)
(Greenwald, 2014)
        Large storage capacity and (“M100 Smart Glasses – Enterprise,” 2015) supports numerous inputs
       Avegant Glyph Gear VR
Google Cardboard
Google Glass
Virtual Reality
Smaller form factor           (“Avegant Glyph,” 2015)
  Cost effective and tethers to android devices Lowest cost and tethers to android devices Monocular and less obtrusive eyewear
(“Samsung Gear VR,” 2015)
(“Google Cardboard,” 2015) (“Google Glass Review,” 2015)
                              Microsoft HoloLens
ODG X6
Mixed Reality
Wide field of view (120 degrees); Full operating system (OS)
Government and industrial use cases
(“Microsoft HoloLens,” 2015) (“ODG,” 2015)
               
Figure 3. Systems Feedback Loop
When hardware can be integrated with toolsets like FACT to understand and communicate design implications, design and feedback activities may occur simultaneously. For example, a user who desires to outfit a vehicle platform with C4I (Command, Control, Communications, Computers, and Intelligence) equipment can start by using the FACT tool to optimize the design against such requirements as power, weight, frequency, and size. The design can then be converted for use by an Oculus Rift headset that allows the user to walk around the vehicle, enter the vehicle, and sit anywhere inside the vehicle, view configured systems and review the design. The same FACT outputs used for the Oculus Rift experience are also used for other augmented reality experience using a mobile phone or tablet device and a freely downloadable application. All of these hardware platforms allow the user to review the design quickly and visually, thereby shortening the feedback loop in complex system design. Cost avoidance is also achieved as multiple prototypes may be eliminated, design time is diminished, and the quality of the final design is increased by including the end user in the design process. As new software interfaces are developed, system design efficiencies will continue to rise.
THE FUTURE OF MIXED REALITY
Mixed Reality is weaving itself into the fabric of DoD acquisition. Following shortly behind the desire to use Mixed Reality for conceptual and design decisions is applications in training, operations, and maintenance. Hardware and software providers are responding to this interest by designing new products to capture the growing market. One example is the use of augmented reality for training applications – overlaying digital information on to real world objects. Information overlay is the key to Mixed Reality and has been derived from heads-up displays utilized by pilots. When the price of wearable technology reaches affordability, adoption will occur quickly. In preparation for this new dimension, new interfaces and middleware are required. If wearable technology, for instance, is to be standard issue for a warfighter in the very near future, the backend software pipeline for Mixed Reality will need to be developed.
New Interfaces Required
Each of the four areas of visualization ingestion requires an interface model to connect with external systems in a scalable way. Instead of directly interfacing with numerous hardware, software, and configuration systems, the visualization engine should exploit linked interfaces. Not only will this decoupled approach maintain maximum
2015 Paper No. 19 Page 6 of 9
MODSIM World 2015
 
scalability, but it will also allow for other areas of focus to emerge. If new areas emerge, an independent interface can be developed and integrated into both the external system and the visualization system.
Currently, the asset interface class exists with a standardized pipeline and process for importing computer-aided design (CAD) models and converting them to light-weight three-dimensional models for use within the Mixed Reality visualization framework. This is the first version of the configuration interface protocol. The other three areas of the architecture (hardware, software, and database interfaces) do not have a mirroring interface protocol in place.
New Middleware Required
In a fully immersive experience, the user is in a virtual environment but feels as if they were in the real world. This experience takes into account more than just sight but employs other immersive computing techniques to trick the human sensory system into virtualizing touch, audio, smell, and taste sensors.
With today’s technology and the evolution of wearable computing and wearable gaming, users can interact in the virtual world with other immersed users and with users in the real world through augmented reality and motion detection cameras. It’s also important to note that these users can interact with each other virtually from any platform, joining the virtual space with a mobile device, web browser, augmented reality glasses, or heads-up display virtual reality device. This exemplifies the platform-independent approach to visualization and adds a digital collaboration capability on top of just the immersive aspect.
The following middleware features enhance and upgrade the platform-independent visualization to optimize the Mixed Reality experience:
o Networking integration to support multiple users
o Mobile support for entry into the interactive space
o Web support for entry into the interactive space
o Heads-up display support for entry into the interactive space o Gaming console support for entry into the interactive space o Virtual reality data and analytics
o Augmented reality support for entry/feedback
o Virtual reality scenario creation from any real world operator or user
o Augmented reality feedback from the interactive space to the real world o Ability for users to submit assets to a central authoritative asset library o Artificial intelligence for virtual reality humanoids, animals, etc.
o Ability for operators to change virtual reality properties and settings
o Ability for operators to create rules and outcomes
MIXED REALITY ENHANCES BETTER BUYING POWER
The overarching initiative from the Under Secretary of Defense for Acquisition Technology and Logistics (USD (AT&L)) to enhance “Better Buying” culture within the DOD acquisition process was released in 2007. In September 2014 an interim Better Buying Power 3.0 was released further defining the process (Kendall, 2014). Mixed Reality supports this process and introduces the need to maintain authoritative data source repositories. These same repositories can provide greater integration across the multitude of divisions that make up the combat development and capability development processes. Mixed Reality capability framework is ideal to support the combat and capability developer well ahead of the acquisition milestones (pre-milestone A) in the development of concepts. Analysts using a standardized web browser can access the Framework for Accessing Cost and Technology (FACT) to reach into a repository of information and data models, and conduct analysis. These data products and technical data packages can then be transferred to subsequent operational behavior modeling where the capability analyst can test their concept and further define their desired capabilities from the analyses conducted in FACT. Integration between FACT and operational behavior modeling can be enhanced by Mixed Reality. Lead analysts can observe in real-time their concept vehicle perform in a constructive model such as the Combined Arms Command
2015 Paper No. 19 Page 7 of 9
MODSIM World 2015

and Control Training Upgrade System (CACCTUS). Figure 4 illustrates this example as part of the Marine Corps Systems Command’s Long Term Vision for Modeling and Simulation Integration.
Figure 4. Marine Corps Systems Command’s Long Term Vision for Modeling & Simulation Integration
Interactions between the different systems from tradespace analysis to constructive simulations to virtual simulators can be expedited in a Mixed Reality perspective to include advances such as virtual immersion via the Oculus Rift. The Mixed Reality framework can share a common 3D CAD modeling technical data package from FACT to enable 3D CAD model agnostic data and information exchange processes for other virtual and augmented reality services to consume. Mixed Reality will enable rapid prototyping as the data is exchanged and consumed across the analytical ecosystem where artifacts are created once and reused from initial concept through the acquisition final operational capability (FOC) of a program lifecycle.
ACKNOWLEDGEMENTS
The authors wish to thank the Marine Corps Systems Command (MCSC) for funding the FACT effort. FACT is under the leadership of Mr. James Smerchansky, Executive Director, MCSC, and under the direction of Mr. Michael O’Neal, Modeling & Simulation (M&S) Division Director, Systems Engineering, Interoperability, Architectures & Technology (SIAT), and Mr. Luis E.Velazquez, Deputy Director, M&S Division SIAT.
REFERENCES
Avegant Glyph. (2015). Retrieved from http://avegant.com/.
DAQRI Smart Helmet. (2015). Retrieved from http://hardware.daqri.com/smarthelmet.
Epson Moverio BT-200 Next Generation Smart Glasses. (2015). Retrieved from http://www.epson.com/cgi- bin/Store/jsp/Landing/moverio-bt-200-smart-glasses.do.
MODSIM World 2015
   2015 Paper No. 19 Page 8 of 9

Google Cardboard. (2015). Retrieved from https://www.google.com/get/cardboard/.
Google Glass Review. (2015). Retrieved from http://www.techradar.com/reviews/gadgets/google-glass- 1152283/review.
Greenwald, W. (2014, July 17). Test Driving Sony’s Project Morpheus. Retrieved from http://www.pcmag.com/article2/0,2817,2460948,00.asp.
Kendall, F. (2014, September 19). Better Buying Power 3.0. Retrieved from http://www.acq.osd.mil/fo/docs/Better_Buying_Power_30-091914.pdf.
Lenuik, Tracy A., & Murley, Samuel R. (2014). Data on Demand: Building a Framework to Support Platform- Independent Visualization (PIV). Paper presented at MODSIM World 2014, Hampton, VA.
M100 Smart Glasses – Enterprise. (2015). Retrieved from http://www.vuzix.com/consumer/products_m100/. Microsoft HoloLens. (2015). Retrieved from http://www.microsoft.com/microsoft-hololens/en-us/get-ready. Oakley Airwave 1.5. (2015). Retrieved from http://www.oakley.com/en/collection/airwave.
ODG. (2015). Retrieved from http://osterhoutgroup.com/home.
Razer OSVR. (2015). Retrieved from http://www.razerzone.com/osvr.
Samsung Gear VR. (2015). Retrieved from http://www.samsung.com/global/microsite/gearvr/.
The All New Oculus Rift Development Kit 2 (DK2) Virtual Reality Headset. (2015). Retrieved from https://www.oculus.com/dk2/.
2015 Paper No. 19 Page 9 of 9
MODSIM World 2015
